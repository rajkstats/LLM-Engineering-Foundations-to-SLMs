{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZwKP3f2HGz9"
      },
      "source": [
        "# Encoder-Decoder Transformer Model from Scratch in PyTorch\n",
        "\n",
        "In today's notebook, we'll be focusing on two major components of transformers:\n",
        "\n",
        "1. The Building Blocks\n",
        "2. Training the Model\n",
        "\n",
        "As a brunt of the in-class time will be spent on the building blocks, we'll leave the training logic as an assignment to complete. We'll be focusing more specifically on training once we start using decoder-only architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjbGDn9SF49m"
      },
      "source": [
        "# How AIM Does Assignments\n",
        "\n",
        "Throughout our time together - we'll be providing a number of assignments. Each assignment will be split into two broad categories:\n",
        "\n",
        "1. Base Assignment - a more conceptual and theory based assignment focused on locking in specific key concepts and learnings.\n",
        "2. Hardmode Assignment - a more programming focused assignment focused on core code-concepts used in transformers.\n",
        "\n",
        "Each assignment will have a few of the following categories of exercises:\n",
        "\n",
        "1. ❓Questions - these will be questions that you will be expected to gather the answer to!\n",
        "2. 🏗️ Activities - these will be work or coding activities meant to reinforce specific concepts or theory components.\n",
        "\n",
        "You are expected to complete all of the activities in your selected notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z43kfJGvlQyA"
      },
      "source": [
        "# The Building Block Fundamentals of Transformer Architecture\n",
        "\n",
        "We're going to start with an example of an encoder-decoder model - the kind found in the classic paper:\n",
        "\n",
        "[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf).\n",
        "\n",
        "We'll walk through each step in code - leveraging the [PyTorch]() library heavily - in order to get an idea of how these models work.\n",
        "\n",
        "While this example notebook could be extended to a sincere usecase - we'll be using a toy dataset, and we will not fully train the model until it converges (under-train), as the full training process might take many days!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sd2KYudl0Hn"
      },
      "source": [
        "## The Desired Architecture\n",
        "\n",
        "![image](https://i.imgur.com/YPjbqW6.png)\n",
        "\n",
        "We'll skip over the diagram for now, and talk through each component in detail!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TPUnyvsuovuP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR8M3oT0l8fM"
      },
      "source": [
        "## Embedding\n",
        "\n",
        "![image](https://i.imgur.com/sFlEZ2e.png)\n",
        "\n",
        "The first step will be do convert our tokenized sequence of inputs into an embedding vector. This allows use to understand a rich amount of information about input sequences and their semantic meanings.\n",
        "\n",
        "As the embedding layer will be training along side the rest of the model - it will allow us to have an excellent vector-representation of the tokens in our dataset.\n",
        "\n",
        "Let's see how it looks in code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ddpQjPJWmXZg"
      },
      "outputs": [],
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int, verbose=False) -> None:\n",
        "    \"\"\"\n",
        "    vocab_size - the size of our vocabulary\n",
        "    d_model - the dimension of our embeddings and the input dimension for our model\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.verbose:\n",
        "      print(f\"Embedding Vector (1st 5 elements): {self.embedding(x)[:5] * math.sqrt(self.d_model)}\")\n",
        "    return self.embedding(x) * math.sqrt(self.d_model) # scale embeddings by square root of d_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Q4WPNALUKR"
      },
      "source": [
        "### ❓Question 1:\n",
        "\n",
        "Given:\n",
        "\n",
        "1. Batch Size = `16`\n",
        "2. Sequence Length = `350`\n",
        "\n",
        "What will the output shape of the `InputEmbeddings` layer be?\n",
        "\n",
        "ANSWER:\n",
        "- class takes two main parameters d_model (Embedding dimension) and vocab_size (No. of unique tokens)\n",
        "- It creates an **nn.Embedding layer** which is essentially a lookup table where each token ID maps to a vector of size **d_model**\n",
        "- Input would be of shape (16, 350) containing token indices\n",
        "- Embedding layer transforms each token into a vector of size d_model\n",
        "- In forward Pass Process, Takes a tensor x of shape (batch_size=16, sequence_length=350) containing token indices\n",
        "- Embedding layer converts each token index into its corresponding vector and these vectors are scaled by multiplying with sqrt(d_model) ( scaling is important for the stability of training in transformer models)\n",
        "- **Output:** Returns a tensor of shape **(batch_size=16, sequence_length=350, d_model)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCx8DYkaxwto"
      },
      "source": [
        "### Test Embedding Layer\n",
        "\n",
        "We'll set up a sample Embedding Layer and then test that it does what we'd expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xl9VdXuYwfVU"
      },
      "outputs": [],
      "source": [
        "def test_input_embeddings_with_example():\n",
        "    # Create a small embedding layer\n",
        "    embed = InputEmbeddings(d_model=512, vocab_size=1000)\n",
        "\n",
        "    # Example sentence tokens (simplified)\n",
        "    tokens = torch.tensor([[1, 2, 3, 4, 5]])  # \"The cat sat down quickly\"\n",
        "\n",
        "    output = embed(tokens)\n",
        "    print(f\"Input shape: {tokens.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(\"\\nExample shows how words are converted to high-dimensional vectors\")\n",
        "\n",
        "    # Run technical test\n",
        "    assert output.shape == (1, 5, 512), f\"Expected shape (1, 5, 512), got {output.shape}\"\n",
        "    print(\"✓ Input Embeddings Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBIElPiqxORa",
        "outputId": "136d6b01-9227-4541-9558-b620f4665af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 5])\n",
            "Output shape: torch.Size([1, 5, 512])\n",
            "\n",
            "Example shows how words are converted to high-dimensional vectors\n",
            "✓ Input Embeddings Test Passed\n"
          ]
        }
      ],
      "source": [
        "# Sets embedding dimension to 512\n",
        "# Creates a sample input of shape [1,5]\n",
        "# Vocabulary size of 1000 (can handle tokens with indices from 0 to 999)\n",
        "# Batch size of 1 (one sentence) and Sequence length of 5 (five tokens)\n",
        "# Each number represents a token ID (like \"The\" = 1, \"cat\" = 2, and on)\n",
        "# Processes the input through embedding layer (One sentence with 5 tokens)\n",
        "# Output shape: (1, 5, 512) [1: still one sentence , 5: still five tokens, 512: each token is now a 512-dimensional vector]\n",
        "\n",
        "test_input_embeddings_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra5KCa1KnfrC"
      },
      "source": [
        "## Positional Encoding\n",
        "\n",
        "![image](https://i.imgur.com/IIA3NK3.png)\n",
        "\n",
        "We need to impart information about where each token is in the sequence, but we aren't using any recurrence or convolutions - the easiest way to encode positional information is to inject positional information into our input embeddings.\n",
        "\n",
        "We're going to use the process outlined in the paper to do this - which is to use a specific combination of functions to add positional information to the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5K3NXh7MoM5D"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, seq_len: int, dropout: float, verbose=False) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.verbose=verbose\n",
        "\n",
        "    positional_embeddings = torch.zeros(seq_len, d_model)\n",
        "    positional_sequence_vector = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    positional_model_vector = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    positional_embeddings[:, 0::2] = torch.sin(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings[:, 1::2] = torch.cos(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings = positional_embeddings.unsqueeze(0)\n",
        "\n",
        "    self.register_buffer('positional_embeddings', positional_embeddings)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.positional_embeddings[:, :x.shape[1], :]).requires_grad_(False)\n",
        "    if self.verbose:\n",
        "      print(f\"Positional Encodings (1st 5 elements): {x}\")\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKGHvs0eMCoQ"
      },
      "source": [
        "### ❓Question 2:\n",
        "\n",
        "Given:\n",
        "\n",
        "1. Batch Size = `16`\n",
        "2. Sequence Length = `350`\n",
        "\n",
        "What will the output shape of the `PositionalEncoding` layer be?\n",
        "\n",
        "# ANSWER\n",
        "- **seq_len**: maximum sequence length and **d_model**: dimension of the embeddings\n",
        "- Creates empty matrix to store positional encodings of shape (seq_len=350, d_model)\n",
        "- Creates vector [0,1,2,...,seq_len-1 (350-1)] and unsqueeze(1) makes it a column vector\n",
        "- Creates frequencies for each dimension and frequencies decrease as dimension increases which helps capture different aspects of position\n",
        "- Even dimensions use sine function and Odd dimensions use cosine function.\n",
        "  This creates unique patterns for each position\n",
        "- In forward pass,it adds positional encodings to input embeddings and applies dropout for regularization\n",
        "- So **Output shape:(16,350,d_model)** preserving the input shape since it is added element wise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkcMmoT2yOZs"
      },
      "source": [
        "### Test Positional Encoding Layer\n",
        "\n",
        "We'll set up a sample Positional Encoding Layer and then test that it does what we'd expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ayoWMbSYySBp"
      },
      "outputs": [],
      "source": [
        "def test_positional_encoding_with_example():\n",
        "    pos = PositionalEncoding(d_model=512, seq_len=10, dropout=0.1)\n",
        "\n",
        "    # Create sample embeddings for \"The cat sat\"\n",
        "    x = torch.randn(1, 3, 512)\n",
        "\n",
        "    output = pos(x)\n",
        "    print(\"Input tokens position:  [1, 2, 3]\")\n",
        "    print(\"Added position info to each word's embedding\")\n",
        "    print(f\"Output maintains shape: {output.shape}\")\n",
        "\n",
        "    # Verify position information was added\n",
        "    assert not torch.allclose(output, x), \"Position information should modify embeddings\"\n",
        "    print(\"✓ Positional Encoding Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4hDXJaGyU5M",
        "outputId": "99f21d0d-c3ef-49fe-ad0d-7c11d2e1c56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokens position:  [1, 2, 3]\n",
            "Added position info to each word's embedding\n",
            "Output maintains shape: torch.Size([1, 3, 512])\n",
            "✓ Positional Encoding Test Passed\n"
          ]
        }
      ],
      "source": [
        "# It first creates an positional encoding layer\n",
        "# Creates sample input where batch size = 1 (one sentence) and seq_len = 3 and each word has a 512 dimensional embedding vector\n",
        "# Values are randomly initialized using randn()\n",
        "# Applies positional encoding\n",
        "# Returns tensor of same shape (1, 3, 512)\n",
        "test_positional_encoding_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueiM7LzKpcFY"
      },
      "source": [
        "## Add & Norm\n",
        "\n",
        "Next we'll tackle the Add & Norm Block of the diagram.\n",
        "\n",
        "![image](https://i.imgur.com/otdEq4D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lDABPLSKqOt"
      },
      "source": [
        "### Layer Normalization\n",
        "\n",
        "The first step is to add layer normalization. You can read more about it [here](https://paperswithcode.com/method/layer-normalization)!\n",
        "\n",
        "The basic idea is that it makes training the model a bit easier, and allows the model to generalize a bit better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1Nlv7BH7ruSf"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, features: int, epsilon:float=10**-6) -> None:\n",
        "    super().__init__()\n",
        "    self.epsilon = epsilon\n",
        "    self.gamma = nn.Parameter(torch.ones(features))\n",
        "    self.beta = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim = -1, keepdim = True)\n",
        "    standard_deviation = x.std(dim = -1, keepdim = True)\n",
        "    return self.gamma * (x - mean) / (standard_deviation + self.epsilon) + self.beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg-deMc3MNnM"
      },
      "source": [
        "### ❓Question 3:\n",
        "\n",
        "What is the purpose of `epsilon` in the above code.\n",
        "\n",
        "> HINT: Pay special attention to the math in the `return` statement.\n",
        "\n",
        "### ANSWER:\n",
        "- `epsilon` is a small constant (typically 10^-6) added to the denominator to prevent division by zero when standard deviation becomes zero which can happen when all input values are identical and allow the network to train properly even with very small variances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vft5kFUBy1aE"
      },
      "source": [
        "### Test Layer Normalization\n",
        "\n",
        "We'll set up a sample Layer Normalization and then test that it does what we'd expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "01Llr9cwy7C2"
      },
      "outputs": [],
      "source": [
        "def test_layer_normalization_with_example():\n",
        "    # features=3 means each word embedding has 3 dimensions\n",
        "    layer_norm = LayerNormalization(features=3)  # Smaller feature size for example\n",
        "\n",
        "    # Simulate word embeddings with different magnitudes\n",
        "    word_embeddings = torch.tensor([\n",
        "        [2.5, 4.1, -3.2],  # \"The\" (high magnitude)\n",
        "        [0.1, 0.2, -0.1],  # \"cat\" (low magnitude)\n",
        "        [8.2, -6.1, 5.5]   # \"sat\" (very high magnitude)\n",
        "    ]).unsqueeze(0)\n",
        "\n",
        "    normalized = layer_norm(word_embeddings)\n",
        "\n",
        "    print(\"Before normalization (magnitudes vary greatly):\")\n",
        "    print(word_embeddings[0])\n",
        "    print(\"\\nAfter normalization (values scaled to similar ranges):\")\n",
        "    print(normalized[0])\n",
        "\n",
        "    # Verify statistical properties\n",
        "    mean = normalized.mean(dim=-1)\n",
        "    var = normalized.var(dim=-1)\n",
        "    assert torch.allclose(mean, torch.zeros_like(mean), atol=1e-5)\n",
        "    assert torch.allclose(var, torch.ones_like(var), atol=1e-5)\n",
        "    print(\"✓ Layer Normalization Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75hToLCry9mR",
        "outputId": "15231a2a-dd42-4ba9-a7d5-644acd234c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before normalization (magnitudes vary greatly):\n",
            "tensor([[ 2.5000,  4.1000, -3.2000],\n",
            "        [ 0.1000,  0.2000, -0.1000],\n",
            "        [ 8.2000, -6.1000,  5.5000]])\n",
            "\n",
            "After normalization (values scaled to similar ranges):\n",
            "tensor([[ 0.3562,  0.7732, -1.1293],\n",
            "        [ 0.2182,  0.8729, -1.0911],\n",
            "        [ 0.7459, -1.1363,  0.3905]], grad_fn=<SelectBackward0>)\n",
            "✓ Layer Normalization Test Passed\n"
          ]
        }
      ],
      "source": [
        "# After normalization, all values are now roughly between -1 and 1\n",
        "# statistical test confirms mean is approximately 0 and variance is 1\n",
        "# This normalization helps stabilize training and reduce the impact of varying magnitudes\n",
        "test_layer_normalization_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwXt7KKKycrl"
      },
      "source": [
        "### Residual Connection\n",
        "\n",
        "Another technique that makes model training easier, we add a Residual connection to the outputs of the Attention Block - this helps to prevent vanishing gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XKwGFD2-yd-Z"
      },
      "outputs": [],
      "source": [
        "# This takes input x\n",
        "# Apply layer normalization\n",
        "# Pass through sublayer (like attention or feed-forward)\n",
        "# Apply dropout\n",
        "# Add back the original input x (the residual connection)\n",
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, features: int, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.layernorm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x + self.dropout(sublayer(self.layernorm(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p668YYgBzc3G"
      },
      "source": [
        "### Testing Residual Connection\n",
        "\n",
        "We'll set up a sample Residual Connection and then test that it does what we'd expect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "heebV5mIzje4"
      },
      "outputs": [],
      "source": [
        "def test_residual_connection_with_example():\n",
        "    residual = ResidualConnection(features=3, dropout=0.1)\n",
        "\n",
        "    # Original input \"The cat\"\n",
        "    x = torch.tensor([\n",
        "        [1.0, 1.0, 1.0],\n",
        "        [2.0, 2.0, 2.0]\n",
        "    ]).unsqueeze(0)\n",
        "\n",
        "    # Sublayer that makes meaningful changes\n",
        "    def sublayer(x):\n",
        "        return torch.nn.functional.relu(x + 0.5) # Non-linear transformation\n",
        "\n",
        "    output = residual(x, sublayer)\n",
        "\n",
        "    print(\"Original input:\")\n",
        "    print(x[0])\n",
        "    print(\"\\nAfter residual connection (combines original + transformed):\")\n",
        "    print(output[0])\n",
        "\n",
        "    # Verify output changed but maintained shape\n",
        "    assert output.shape == x.shape\n",
        "    assert torch.any(torch.abs(output - x) > 1e-6), \"Output should differ from input\"\n",
        "    print(\"✓ Residual Connection Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZMF65cmzmgx",
        "outputId": "544fb86f-6328-48b9-e330-676d5e4430c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original input:\n",
            "tensor([[1., 1., 1.],\n",
            "        [2., 2., 2.]])\n",
            "\n",
            "After residual connection (combines original + transformed):\n",
            "tensor([[1.5556, 1.5556, 1.5556],\n",
            "        [2.5556, 2.5556, 2.5556]], grad_fn=<SelectBackward0>)\n",
            "✓ Residual Connection Test Passed\n"
          ]
        }
      ],
      "source": [
        "# It takest two words each with 3-dimensional embeddings\n",
        "# defines a sublayer which adds 0.5 to all values and applies ReLU which sets negative values to 0\n",
        "# Finally, adds original input and residual connection\n",
        "# We can see that values increased from original due to the +0.5 in sublayer while shape remained the same 2 words and 3 dimension\n",
        "test_residual_connection_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIOZp3xhsaXK"
      },
      "source": [
        "## Feed Forward Network\n",
        "\n",
        "![image](https://i.imgur.com/woEqBjQ.png)\n",
        "\n",
        "Moving onto the next component, we have our feed forward network.\n",
        "\n",
        "The feed forward networks servers two purposes in our model:\n",
        "\n",
        "1. It reforms the attention outputs into a format that works with the next block.\n",
        "\n",
        "2. It helps add complexity to prevent each attention block acting in a similar fashion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JueNG2UBszbR"
      },
      "outputs": [],
      "source": [
        "# For input shape (batch_size, seq_len, d_model)\n",
        "# Linear1 expands dimensionality (e.g., 512 → 2048)\n",
        "# Dropout prevents overfitting\n",
        "# Linear2 brings back to original dimension (2048 → 512)\n",
        "# ReLU adds non-linearity\n",
        "# Basically, it temporarily expands to higher dimension which allows more complex feature interactions and returns to original dimension for next layer\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1) -> None:\n",
        "    \"\"\"\n",
        "    d_model - dimension of model\n",
        "    d_ff - dimension of feed forward network\n",
        "    dropout - regularization measure\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU-H1m4L0UhY"
      },
      "source": [
        "### Testing the Feed-forward Block\n",
        "\n",
        "Let's test!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pwtbsJou0fxz"
      },
      "outputs": [],
      "source": [
        "def test_feed_forward_block_with_example():\n",
        "   ff_block = FeedForwardBlock(d_model=3, d_ff=8)  # Small dimensions for demonstration\n",
        "\n",
        "   # Input: Word embeddings for \"The cat\"\n",
        "   x = torch.tensor([\n",
        "       [1.0, 0.5, 0.2],  # \"The\"\n",
        "       [2.0, -0.3, 1.1]  # \"cat\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   output = ff_block(x)\n",
        "\n",
        "   print(\"Input embeddings:\")\n",
        "   print(x[0])\n",
        "   print(\"\\nAfter feed-forward transformation:\")\n",
        "   print(output[0])\n",
        "\n",
        "   # First linear layer expands to d_ff dimensions\n",
        "   # ReLU keeps only positive values\n",
        "   # Second linear layer projects back to d_model dimensions\n",
        "   assert output.shape == x.shape\n",
        "   assert not torch.allclose(output, x)\n",
        "   print(\"✓ Feed Forward Block Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMHE9fWB0hj3",
        "outputId": "31099f1b-2d5d-447d-b500-04148f47e2fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input embeddings:\n",
            "tensor([[ 1.0000,  0.5000,  0.2000],\n",
            "        [ 2.0000, -0.3000,  1.1000]])\n",
            "\n",
            "After feed-forward transformation:\n",
            "tensor([[-0.2921,  0.1775,  0.0933],\n",
            "        [-0.2531,  0.1482,  0.1740]], grad_fn=<SelectBackward0>)\n",
            "✓ Feed Forward Block Test Passed\n"
          ]
        }
      ],
      "source": [
        "# Input dimension:3 , Hidden dimension: 8\n",
        "# Two words, each with 3-dimensional embeddings and unsqueeze(0) adds batch dimension\n",
        "# Linear1: expands from 3→8 dimensions\n",
        "# ReLU: zeros out negative values\n",
        "# Dropout: randomly zeros some values\n",
        "# Linear2: contracts from 8→3 dimensions\n",
        "# In the output, Shape is preserved but values are completely transformed\n",
        "\n",
        "test_feed_forward_block_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ5EGkvdtP0O"
      },
      "source": [
        "## Multi-Head Attention\n",
        "\n",
        "![image](https://i.imgur.com/4qOT46y.png)\n",
        "\n",
        "Next up is the heart and soul of the Transformer - Multi-Head Attention.\n",
        "\n",
        "We'll break it down into the basic building blocks in code in the following section!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Oto52ZyvX0k"
      },
      "source": [
        "### Multi-Head Attention Class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mDHkw1f_vmuv"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    assert d_model % num_heads == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout: nn.Dropout = None):\n",
        "    d_k = query.shape[-1]\n",
        "\n",
        "    attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "    attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    return (attention_scores @ value), attention_scores\n",
        "\n",
        "  def forward(self, query, key, value, mask):\n",
        "    query = self.w_q(query)\n",
        "    key = self.w_k(key)\n",
        "    value = self.w_v(value)\n",
        "\n",
        "    query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "    return self.w_o(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEm7I6iwMiom"
      },
      "source": [
        "### ❓Question 4:\n",
        "\n",
        "What do: Q, K, V, and O stand for in the above code?\n",
        "\n",
        "### ANSWER:\n",
        "- **Q - Query** : Represents what we're asking or searching for in the attention mechanism\n",
        "- **K - Key**: Represents what we're matching against\n",
        "- **V - Value**:Represents the actual content we want to retrieve\n",
        "- **O - Output**: Combines and transforms the attention results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dp3eM7H1cRl"
      },
      "source": [
        "### Testing Multi-Head Attention\n",
        "\n",
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "O6R_o-c-1fCu"
      },
      "outputs": [],
      "source": [
        "def test_multi_head_attention_with_example():\n",
        "   mha = MultiHeadAttention(d_model=6, num_heads=2)  # Small dimensions for clarity\n",
        "\n",
        "   # Input sequence: [\"The\", \"cat\", \"sat\"]\n",
        "   query = key = value = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"The\"\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
        "       [0.0, 0.0, 0.0, 0.0, 1.0, 1.0]   # \"sat\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   # Allow all words to attend to each other\n",
        "   mask = torch.ones(1, 1, 3, 3)\n",
        "\n",
        "   output = mha(query, key, value, mask)\n",
        "\n",
        "   print(\"Input embeddings (each row is a word):\")\n",
        "   print(query[0])\n",
        "   print(\"\\nAttention output (words now contain mixed information from relevant words):\")\n",
        "   print(output[0])\n",
        "\n",
        "   # Each head processes sequence differently, then results are combined\n",
        "   assert output.shape == query.shape\n",
        "   assert not torch.allclose(output, query)\n",
        "   print(\"✓ Multi-Head Attention Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owCjHXhi1gYL",
        "outputId": "eccde90a-2530-4e1e-d91f-32ec02ce78ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input embeddings (each row is a word):\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 1.]])\n",
            "\n",
            "Attention output (words now contain mixed information from relevant words):\n",
            "tensor([[ 0.1751, -0.0509, -0.0940, -0.0794, -0.0006,  0.1361],\n",
            "        [ 0.1943, -0.2063, -0.1681, -0.0521, -0.0122,  0.0943],\n",
            "        [ 0.1258,  0.0933, -0.0557, -0.0144, -0.1042,  0.0453]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "✓ Multi-Head Attention Test Passed\n"
          ]
        }
      ],
      "source": [
        "# Model dimension = 6 and using 2 attention heads (each head works with 3 dimensions\n",
        "# Each word has a 6-dimensional embedding\n",
        "# Clear pattern where each word uses different positions\n",
        "# All ones means each word can attend to all other words\n",
        "# Shape (1, 1, 3, 3) for batch, heads, sequence, sequence\n",
        "\n",
        "# Output values show mixing of information across all positions\n",
        "# Input and output maintain same shape (3 words × 6 dimensions)\n",
        "# Each word now has a representation influenced by all other words\n",
        "test_multi_head_attention_with_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyD7nAM6tb0K"
      },
      "source": [
        "### Scaled Dot-Product Attention\n",
        "\n",
        "![image](https://i.imgur.com/Yp48DuB.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dKk08SvowJIc"
      },
      "outputs": [],
      "source": [
        "# Performs dot product between query and key matrices\n",
        "# Scaling by √d_k prevents dot products from getting too large\n",
        "# Result shows how much each word should attend to other words\n",
        "\n",
        "# Applies mask to prevent attention to certain positions\n",
        "# Sets masked positions to very negative number (-1e9)\n",
        "# After softmax, masked positions will be close to 0\n",
        "\n",
        "# Converts scores to probabilities between 0 and 1\n",
        "# Each row sums to 1\n",
        "# Higher scores get higher probabilities\n",
        "\n",
        "# Dropout: Randomly zeros out some attention scores and helps prevent overfitting\n",
        "# Multiplies attention probabilities with values\n",
        "# Returns both output and attention scores\n",
        "\n",
        "def attention(query, key, value, mask, d_k, dropout: nn.Dropout = None):\n",
        "  attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "    attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "  attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "  if dropout is not None:\n",
        "    attention_scores = dropout(attention_scores)\n",
        "\n",
        "  return (attention_scores @ value), attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac8k7b3SxKOC"
      },
      "source": [
        "### Forward Method\n",
        "\n",
        "This is code is required to do a forward pass with our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "um2Oxv1axWYK"
      },
      "outputs": [],
      "source": [
        "# Transforms inputs into query, key, and value representations\n",
        "# Reshape for Multiple Heads\n",
        "# view: Reshapes tensor to split last dimension for multiple heads\n",
        "# transpose(1, 2): Moves head dimension before sequence length\n",
        "# Input: (batch, seq_len, d_model)\n",
        "# After view: (batch, seq_len, num_heads, d_k)\n",
        "# After transpose: (batch, num_heads, seq_len, d_k)\n",
        "# Applies scaled dot-product attention\n",
        "# Each head processes its portion independently\n",
        "# Returns attended values and attention scores\n",
        "\n",
        "def forward(self, query, key, value, mask):\n",
        "  query = self.w_q(query)\n",
        "  key = self.w_k(key)\n",
        "  value = self.w_v(value)\n",
        "\n",
        "  query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "  key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "  value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "  x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "  x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "  return self.w_o(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgr_D0mmyEgp"
      },
      "source": [
        "### Combining it All Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "odYKdmwMyH4P"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    assert d_model % num_heads == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout: nn.Dropout = None):\n",
        "    d_k = query.shape[-1]\n",
        "\n",
        "    attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "    attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    return (attention_scores @ value), attention_scores\n",
        "\n",
        "  def forward(self, query, key, value, mask):\n",
        "    query = self.w_q(query)\n",
        "    key = self.w_k(key)\n",
        "    value = self.w_v(value)\n",
        "\n",
        "    query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "    return self.w_o(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRFqoZyD1-AU"
      },
      "source": [
        "### Testing MultiHeadAttention\n",
        "\n",
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sY1N60di2CHQ"
      },
      "outputs": [],
      "source": [
        "def test_attention_mechanism():\n",
        "   mha = MultiHeadAttention(d_model=6, num_heads=2)\n",
        "\n",
        "   # Simple sequence: \"The cat sleeps\"\n",
        "   seq = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],\n",
        "       [0.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n",
        "   ]).unsqueeze(0)  # [1, 3, 6]\n",
        "\n",
        "   # Mask shape needs to match attention scores [batch, heads, seq_len, seq_len]\n",
        "   attention_scores = torch.ones(1, 2, 3, 3)  # 2 heads, sequence length 3\n",
        "\n",
        "   print(\"Input sequence shape:\", seq.shape)\n",
        "   print(\"Input values (each row is a word):\")\n",
        "   print(seq[0])\n",
        "\n",
        "   output = mha(seq, seq, seq, attention_scores)\n",
        "   print(\"\\nOutput after attention:\")\n",
        "   print(output[0])\n",
        "\n",
        "   # Verify output maintains shape but changes values\n",
        "   assert output.shape == seq.shape\n",
        "   assert not torch.allclose(output, seq)\n",
        "   print(\"✓ Multi-Head Attention Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qEqn4BS2DHt",
        "outputId": "5c587e87-98fb-48d2-ac9b-eb2676f04486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence shape: torch.Size([1, 3, 6])\n",
            "Input values (each row is a word):\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 1.]])\n",
            "\n",
            "Output after attention:\n",
            "tensor([[-0.1178,  0.0958,  0.2017,  0.0754, -0.0320, -0.1525],\n",
            "        [-0.0940,  0.1834,  0.2268,  0.1714,  0.0987, -0.0842],\n",
            "        [-0.1140,  0.1045,  0.1791,  0.0717, -0.0326, -0.1154]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "✓ Multi-Head Attention Test Passed\n"
          ]
        }
      ],
      "source": [
        "# Creates attention layer with 6 dimensions split into 2 heads\n",
        "# Each head processes 3 dimensions (d_k = 6/2 = 3)\n",
        "# Converts input to Q, K, V representations\n",
        "# Splits 6D into 2 heads of 3D each\n",
        "# Shape becomes (1, 2, 3, 3)\n",
        "# Calculates attention scores\n",
        "# Recombines heads and projects back to original dimensions weighted combinations of values\n",
        "\n",
        "# Original clear position patterns are transformed\n",
        "# Each word now contains mixed information from other words\n",
        "\n",
        "test_attention_mechanism()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkKjLhpiyz5b"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "When we pass information through our model - the first thing we will do is Encode it by passing it through our Encoder Blocks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-0edIfMzijj"
      },
      "source": [
        "### Encoder Block\n",
        "\n",
        "![image](https://i.imgur.com/nwNYZAT.png)\n",
        "\n",
        "The encoder takes in the source language sentence (e.g. English). Each word is converted into a vector representation using an embedding layer. Then a positional encoder adds information about the position of each word. This goes through multiple self-attention layers, where each word vector attends to all other word vectors to build contextual representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dMVnZiGDy1PG"
      },
      "outputs": [],
      "source": [
        "# Takes four main components\n",
        "# features: dimension of model, self_attention_block: multi-head attention layer, feed_forward_block: feed-forward network, dropout: for regularization\n",
        "# Creates two residual connections:for self-attention output and for feed-forward output and each includes layer normalization and dropout\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, features: int, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, input_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, input_mask))\n",
        "    x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_twqqReS28ul"
      },
      "source": [
        "### Testing the EncoderBlock\n",
        "\n",
        "Testing time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "PP_wq0R02_g0"
      },
      "outputs": [],
      "source": [
        "def test_encoder_block():\n",
        "   # Create encoder block with small dimensions\n",
        "   mha = MultiHeadAttention(d_model=6, num_heads=2)\n",
        "   ff = FeedForwardBlock(d_model=6, d_ff=12)\n",
        "   encoder = EncoderBlock(features=6, self_attention_block=mha, feed_forward_block=ff, dropout=0.1)\n",
        "\n",
        "   # Input: \"The cat sleeps\"\n",
        "   x = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"The\"\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
        "       [0.0, 0.0, 0.0, 0.0, 1.0, 1.0]   # \"sleeps\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   # Attention mask\n",
        "   mask = torch.ones(1, 2, 3, 3)  # Allow all connections\n",
        "\n",
        "   output = encoder(x, mask)\n",
        "\n",
        "   print(\"Input sequence:\")\n",
        "   print(x[0])\n",
        "   print(\"\\nAfter encoder processing (self-attention + feed-forward):\")\n",
        "   print(output[0])\n",
        "\n",
        "   assert output.shape == x.shape\n",
        "   assert not torch.allclose(output, x)\n",
        "   print(\"✓ Encoder Block Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPAH-NKA3AEN",
        "outputId": "70158090-5e61-4bc4-d802-27004bdc10cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 1.]])\n",
            "\n",
            "After encoder processing (self-attention + feed-forward):\n",
            "tensor([[ 1.0704,  0.5517, -0.5197,  0.2174, -0.5780, -0.2566],\n",
            "        [-0.0539, -0.0045,  0.6857,  0.7902,  0.0653, -0.2997],\n",
            "        [-0.1276, -0.2211, -0.0289, -0.1322,  1.1788,  0.8078]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "✓ Encoder Block Test Passed\n"
          ]
        }
      ],
      "source": [
        "# creates a small encoder with 6-dimensional model space and 2 attention heads\n",
        "# feed-forward network that expands to 12 dimensions internally\n",
        "# 10% dropout for regularization\n",
        "# Each word's representation now spans all dimensions, showing information mixing\n",
        "# Some values have become negative, showing complex transformations\n",
        "test_encoder_block()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-AvnoPrzvwu"
      },
      "source": [
        "### Encoder Stack\n",
        "\n",
        "Following along from the original paper - we will organize these blocks into a set of 6.\n",
        "\n",
        "These 6 Encoder Blocks (each with 8 Attention Heads) will comprise our Encoding Stack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kOaR5SjUzxv7"
      },
      "outputs": [],
      "source": [
        "# Takes a list of encoder blocks\n",
        "# Each block is identical in structure but has different learned parameters\n",
        "# Adds final layer normalization\n",
        "# Each encoder block processes the output of the previous block\n",
        "# Builds increasingly sophisticated representations and maintains input dimensions throughout\n",
        "# Same attention mask used through all layers\n",
        "# Normalizes the final output\n",
        "# Helps with numerical stability\n",
        "\n",
        "class EncoderStack(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQyujsRTz5NT"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "Next, we will take the encoded sequence and decode it through our Decoder Blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBYgl77Kz6bx"
      },
      "source": [
        "### Decoder Block\n",
        "\n",
        "![image](https://i.imgur.com/HtAAXZc.png)\n",
        "\n",
        "The decoder takes in the target language sentence (e.g. Italian). It also converts words to vectors and adds positional info. Then it goes through self-attention layers. Here, a mask is applied so each word can only see the words before it, not after.\n",
        "\n",
        "The decoder also does attention over the encoder output. This allows each French word to find relevant connections with the English words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SIwafbqzz5-n"
      },
      "outputs": [],
      "source": [
        "# From Encoder, Has two attention blocks instead of one\n",
        "# and Uses three residual connections instead of two\n",
        "# self_attention_block: Attention within target sequence\n",
        "# cross_attention_block: Attention between target and source sequences\n",
        "# Both are MultiHeadAttention but used differently\n",
        "# Forward Pass Processing\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, features: int, self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "  def forward(self, x, encoder_output, input_mask, target_mask):\n",
        "    # Self-attention with target mask\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, target_mask))\n",
        "    # Cross-attention with input mask\n",
        "    x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, input_mask))\n",
        "    # Feed-forward processing\n",
        "    x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0KkwjkM3xe7"
      },
      "source": [
        "### Testing DecoderBlock\n",
        "\n",
        "You know what's up next...testing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UnmnWAVl307S"
      },
      "outputs": [],
      "source": [
        "def test_decoder_block():\n",
        "   # Initialize components with small dimensions\n",
        "   self_attn = MultiHeadAttention(d_model=6, num_heads=2)\n",
        "   cross_attn = MultiHeadAttention(d_model=6, num_heads=2)\n",
        "   ff = FeedForwardBlock(d_model=6, d_ff=12)\n",
        "   decoder = DecoderBlock(features=6, self_attention_block=self_attn,\n",
        "                         cross_attention_block=cross_attn,\n",
        "                         feed_forward_block=ff, dropout=0.1)\n",
        "\n",
        "   # Input: \"El gato\" (target sequence)\n",
        "   x = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"El\"\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"gato\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   # Encoder output: \"The cat\" (source sequence)\n",
        "   encoder_output = torch.tensor([\n",
        "       [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"The\"\n",
        "       [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   # Masks\n",
        "   src_mask = torch.ones(1, 2, 2, 2)  # Can attend to all encoder outputs\n",
        "   tgt_mask = torch.tril(torch.ones(1, 2, 2, 2))  # Can only attend to previous words\n",
        "\n",
        "   output = decoder(x, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "   print(\"Input target sequence:\")\n",
        "   print(x[0])\n",
        "   print(\"\\nSource (encoder) sequence:\")\n",
        "   print(encoder_output[0])\n",
        "   print(\"\\nDecoder output (after self-attention, cross-attention, and feed-forward):\")\n",
        "   print(output[0])\n",
        "\n",
        "   assert output.shape == x.shape\n",
        "   assert not torch.allclose(output, x)\n",
        "   print(\"✓ Decoder Block Test Passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COJlCq3033A2",
        "outputId": "e860e161-cf3d-4426-94c7-0bf9762f907f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input target sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.]])\n",
            "\n",
            "Source (encoder) sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.]])\n",
            "\n",
            "Decoder output (after self-attention, cross-attention, and feed-forward):\n",
            "tensor([[ 1.0401,  1.1341,  0.6068,  0.4775, -0.6524, -0.7533],\n",
            "        [ 0.2025, -0.0810,  1.1004,  1.1390,  0.0200, -0.3606]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "✓ Decoder Block Test Passed\n"
          ]
        }
      ],
      "source": [
        "# Creates a decoder with two attention mechanisms (self and cross), Feed-forward network, 6-dimensional model space and 2 attention heads\n",
        "# src_mask allows full access to encoder output\n",
        "# tgt_mask prevents looking at future words\n",
        "# Values spread across all dimensions and shows influence from encoder information\n",
        "\n",
        "test_decoder_block()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa4yiTNn0BkA"
      },
      "source": [
        "### Decoder Stack\n",
        "\n",
        "We'll use the same number of Decoder Blocks as we did Encoder Blocks - leaving us with 6 Deocder Blocks in our Decoder Stack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1QUXzOXk0CcT"
      },
      "outputs": [],
      "source": [
        "# Takes list of 6 decoder blocks\n",
        "# Each block identical in structure but with different parameters\n",
        "# Final normalization layer\n",
        "\n",
        "class DecoderStack(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, encoder_output, input_mask, target_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, input_mask, target_mask)\n",
        "    return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRFiAP580S4b"
      },
      "source": [
        "## Linear Projection Layer\n",
        "\n",
        "After the decoder's self-attention and encoder-decoder attention layers, we have a context vector representing each Italian word. This context vector has a high dimension (e.g. 512 or 1024).\n",
        "\n",
        "We want to take this context vector and generate a probability distribution over the French vocabulary so we can pick the next translated word.\n",
        "\n",
        "The linear projection layer helps with this. It projects the context vector into a much larger vector called the vocabulary distribution - one entry per word in the vocabulary.\n",
        "\n",
        "For example, if our Italian vocabulary has 50,000 words, the vocabulary distribution will have 50,000 dimensions. Each dimension corresponds to the probability of that Italian word being the correct translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tkBBMAZK0WLB"
      },
      "outputs": [],
      "source": [
        "# Takes two parameters d_model: Input dimension (ex: 512) and vocab_size: Output dimension (ex: 50,000)\n",
        "# Creates single linear transformation\n",
        "# Enables word prediction\n",
        "\n",
        "class LinearProjectionLayer(nn.Module):\n",
        "  def __init__(self, d_model, vocab_size) -> None:\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x) -> None:\n",
        "    return self.proj(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9ucsRWs0lG9"
      },
      "source": [
        "## The Transformer\n",
        "\n",
        "At this point, all we need to do is create a class that represents our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5ip11mmQ0nMM"
      },
      "outputs": [],
      "source": [
        "# Transformer takes Encoder and Decoder blocks, Source and Target embeddings, Source and Target positional encodings and Final projection layer\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder: EncoderBlock, decoder: DecoderBlock, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: LinearProjectionLayer) -> None:\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.src_pos = src_pos\n",
        "    self.tgt_pos = tgt_pos\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    #Convert tokens to embeddings\n",
        "    src = self.src_embed(src)\n",
        "    #Add positional information\n",
        "    src = self.src_pos(src)\n",
        "    #Process through encoder\n",
        "    return self.encoder(src, src_mask)\n",
        "\n",
        "  def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "     # Convert target tokens to embeddings\n",
        "    tgt = self.tgt_embed(tgt)\n",
        "    # Add positional information\n",
        "    tgt = self.tgt_pos(tgt)\n",
        "    return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "  def project(self, x):\n",
        "    # Converts decoder output to vocabulary probabilities\n",
        "    return self.projection_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ssr5nA039--"
      },
      "source": [
        "## Building Our Transformer\n",
        "\n",
        "Now that we have each of our components - we need to construct an actual model!\n",
        "\n",
        "We'll use this helper function to aid in our goal and set up our Encoder/Decoder Stacks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "kdCt3wNi4EvY"
      },
      "outputs": [],
      "source": [
        "def build_transformer(input_vocab_size: int, target_vocab_size: int, input_seq_len: int, target_seq_len: int, d_model: int=512, N: int=6, num_heads: int=8, dropout: float=0.1, d_ff: int=2048, verbose=True) -> Transformer:\n",
        "  # Creates embedding layers for both source and target languages\n",
        "  input_embeddings = InputEmbeddings(d_model, input_vocab_size, verbose=verbose)\n",
        "  target_embeddings = InputEmbeddings(d_model, target_vocab_size)\n",
        "\n",
        "  # Adds position information to embeddings\n",
        "  input_position = PositionalEncoding(d_model, input_seq_len, dropout, verbose=verbose)\n",
        "  target_position = PositionalEncoding(d_model, target_seq_len, dropout)\n",
        "\n",
        "  # Encoder Stack Creation\n",
        "  encoder_blocks = []\n",
        "\n",
        "  for _ in range(N):\n",
        "    encoder_self_attention_block = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "    encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "    encoder_blocks.append(encoder_block)\n",
        "\n",
        "  # Decoder Stack Creation\n",
        "  decoder_blocks = []\n",
        "\n",
        "  for _ in range(N):\n",
        "    decoder_self_attention_block = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    decoder_cross_attention_block = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "    decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "    decoder_blocks.append(decoder_block)\n",
        "\n",
        "  # Combines blocks into complete stacks\n",
        "  encoder_stack = EncoderStack(d_model, nn.ModuleList(encoder_blocks))\n",
        "  decoder_stack = DecoderStack(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "  # Creates final projection to vocabulary\n",
        "  linear_projection_layer = LinearProjectionLayer(d_model, target_vocab_size)\n",
        "\n",
        "  transformer = Transformer(encoder_stack, decoder_stack, input_embeddings, target_embeddings, input_position, target_position, linear_projection_layer)\n",
        "\n",
        "  # Initializes weights using Xavier uniform initialization which helps with training stability\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2jfvmRx4ZNd"
      },
      "source": [
        "# Training Our Transformer!\n",
        "\n",
        "We will be using the resources created in [this](https://github.com/hkproj/pytorch-transformer/tree/main) repository to train our model on a English -> French translation task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxF6JLcg5LJp"
      },
      "source": [
        "## Dataset Creation\n",
        "\n",
        "The BilingualDataset is a custom PyTorch dataset for working with translation data. It needs a tokenizer for each language, a dataset of sentence pairs, info on which languages are source and target, and the max sequence length.\n",
        "\n",
        "This class handles tokenizing the sentences, padding them to be the same length, and getting the data into the right format for sequence-to-sequence models. It adds special start, end, and padding tokens so all the inputs and outputs are the same length.\n",
        "\n",
        "When you grab a sample from the dataset, it tokenizes the source and target sentences, pads them, and creates the input tensors the model needs - encoder input, decoder input, and target labels. It also makes masks to show what's real data vs padding, and to make sure the decoder predictions only use previous tokens, not future ones.\n",
        "\n",
        "The BilingualDataset gets the data ready for training seq2seq models in a way that works with the sequential nature of language. The model can only predict the next token based on what came before it, not after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "WLnUZRgk7S-G"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "  def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "    super().__init__()\n",
        "    self.seq_len = seq_len\n",
        "\n",
        "    self.ds = ds\n",
        "    self.tokenizer_src = tokenizer_src\n",
        "    self.tokenizer_tgt = tokenizer_tgt\n",
        "    self.src_lang = src_lang\n",
        "    self.tgt_lang = tgt_lang\n",
        "\n",
        "    self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "    self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "    self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ds)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    src_target_pair = self.ds[idx]\n",
        "    src_text = src_target_pair['translation'][self.src_lang]\n",
        "    tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "    enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "    dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "    enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
        "    dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "    if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "        raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "    encoder_input = torch.cat(\n",
        "        [\n",
        "            self.sos_token,\n",
        "            torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "            self.eos_token,\n",
        "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    decoder_input = torch.cat(\n",
        "        [\n",
        "            self.sos_token,\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    label = torch.cat(\n",
        "        [\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "            self.eos_token,\n",
        "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    assert encoder_input.size(0) == self.seq_len\n",
        "    assert decoder_input.size(0) == self.seq_len\n",
        "    assert label.size(0) == self.seq_len\n",
        "\n",
        "    return {\n",
        "        \"encoder_input\": encoder_input,\n",
        "        \"decoder_input\": decoder_input,\n",
        "        \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
        "        \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
        "        \"label\": label,\n",
        "        \"src_text\": src_text,\n",
        "        \"tgt_text\": tgt_text,\n",
        "    }\n",
        "\n",
        "def causal_mask(size):\n",
        "  mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "  return mask == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0hSYI7F8IAI"
      },
      "source": [
        "## Build Tokenizer For Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfmUwnpo8qou",
        "outputId": "3375f591-a7c5-4e27-f404-bc40eb1b66e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers tokenizers datasets -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TCDOGq0UD1W"
      },
      "source": [
        "This will grab all the sentences from our dataset per language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "iAUUEXFz8eGH"
      },
      "outputs": [],
      "source": [
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUDKOCTVUIa7"
      },
      "source": [
        "We'll quickly train a tokenizer on our dataset for both our source and target languages.\n",
        "\n",
        "We'll be sure to add the `[UNK]`, `[PAD]`, `[SOS]`, and `[EOS]` special tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "4s1ZHzKb8hTN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "def build_tokenizer(config, ds, lang):\n",
        "  tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "  tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "  return tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEUDihkcVRq1"
      },
      "source": [
        "Now we can create our dataset in a format that our model expects and can train with!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "yI7xXnLo84cE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "def get_ds(config):\n",
        "  # It only has the train split, so we divide it overselves\n",
        "  ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
        "\n",
        "  # Build tokenizers\n",
        "  tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "  tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "  # Keep 90% for training, 10% for validation\n",
        "  train_ds_size = int(0.9 * len(ds_raw))\n",
        "  val_ds_size = len(ds_raw) - train_ds_size\n",
        "  train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "  train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "  val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "  # Find the maximum length of each sentence in the source and target sentence\n",
        "  max_len_src = 0\n",
        "  max_len_tgt = 0\n",
        "\n",
        "  for item in ds_raw:\n",
        "    src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "    tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "    max_len_src = max(max_len_src, len(src_ids))\n",
        "    max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "  print(f'Max length of source sentence: {max_len_src}')\n",
        "  print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "\n",
        "  train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "  val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "  return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr3Ys-ufVW1E"
      },
      "source": [
        "We can build our model with this helper function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "FK6k5X829JOo"
      },
      "outputs": [],
      "source": [
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "  model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'], verbose=False)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "JCALGXpf9tnv"
      },
      "outputs": [],
      "source": [
        "def get_weights_file_path(config, epoch: str):\n",
        "  model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "  model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
        "  return str(Path('.') / model_folder / model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "QPWZgCwD9vVX"
      },
      "outputs": [],
      "source": [
        "def latest_weights_file_path(config):\n",
        "  model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "  model_filename = f\"{config['model_basename']}*\"\n",
        "  weights_files = list(Path(model_folder).glob(model_filename))\n",
        "  if len(weights_files) == 0:\n",
        "      return None\n",
        "  weights_files.sort()\n",
        "  return str(weights_files[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNjAS-LvV557"
      },
      "source": [
        "Finally....our training loop!\n",
        "\n",
        "We'll spend more time in following weeks discussing this - for now, we'll quickly walk through what's happening:\n",
        "\n",
        "1. Configure the training device (GPU/CPU) and print details. Set device in PyTorch.\n",
        "\n",
        "2. Create directory for saving model weights based on config.\n",
        "\n",
        "3. Get data loaders, tokenizers, and model. Move model to configured device.\n",
        "\n",
        "4. Initialize Adam optimizer with learning rate and epsilon from config.\n",
        "\n",
        "5. Set up initial training parameters like start epoch and global step.\n",
        "\n",
        "6. Define cross-entropy loss function with label smoothing, ignoring padding.\n",
        "\n",
        "---\n",
        "\n",
        "- Main training loop over epochs:\n",
        "\n",
        "  - Clear cache, set model to train mode, initialize progress bar.\n",
        "\n",
        "  - For each batch:\n",
        "\n",
        "    - Move data to device, run model forward/backward passes.\n",
        "    - Compute loss, backprop, update model weights.\n",
        "    - Increment global step.\n",
        "  - After each epoch, save model and optimizer checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "gL1bH7is9OfS"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def train_model(config):\n",
        "  # Define the device\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
        "  print(\"Using device:\", device)\n",
        "  if (device == 'cuda'):\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "    print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "  else:\n",
        "    print(\"Please ensure you're in a GPU enabled Colab Notebook instance.\")\n",
        "  device = torch.device(device)\n",
        "\n",
        "  # Make sure the weights folder exists\n",
        "  Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "  model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "  initial_epoch = 0\n",
        "  global_step = 0\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "\n",
        "  for epoch in range(initial_epoch, config['num_epochs']):\n",
        "    torch.cuda.empty_cache()\n",
        "    model.train()\n",
        "    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "    for batch in batch_iterator:\n",
        "      encoder_input = batch['encoder_input'].to(device)\n",
        "      decoder_input = batch['decoder_input'].to(device)\n",
        "      encoder_mask = batch['encoder_mask'].to(device)\n",
        "      decoder_mask = batch['decoder_mask'].to(device)\n",
        "\n",
        "      encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "      decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "      proj_output = model.project(decoder_output)\n",
        "\n",
        "      label = batch['label'].to(device)\n",
        "\n",
        "      loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "      batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "      global_step += 1\n",
        "\n",
        "    model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "    torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'global_step': global_step\n",
        "    }, model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "f21pxGoS9-gM"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "  \"batch_size\": 64,\n",
        "  \"num_epochs\": 6,\n",
        "  \"lr\": 1e-4,\n",
        "  \"seq_len\": 350,\n",
        "  \"d_model\": 512,\n",
        "  \"datasource\": 'opus_books',\n",
        "  \"lang_src\": \"en\",\n",
        "  \"lang_tgt\": \"it\",\n",
        "  \"model_folder\": \"trained__en_it_translation_model\",\n",
        "  \"model_basename\": \"encoder_decoder_model_\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "hfBcSHUo-MU6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "0873e2d0d80c4587a6fe94a334eb6d20",
            "04dd4c00fd1a42fdae52a62f0a2fde1b",
            "7267189d27a741b39f54c2aa8fe6f3db",
            "7a84ac6f52444198b0a8dc06a6597cfb",
            "4828e7291a754eafa48d7d9da724c7c8",
            "e6472ac80ec44a658a106ff6939179e6",
            "8cd613d4fd56464c8685db4916f74233",
            "1af5418e4a1e44afab78916932d647ba",
            "b32028e3d5104cb0aa1dde22e2c02a4f",
            "d5f12dc803a54dc8992b530bf4a05d27",
            "07cb748d7e1f431b85a4c8b1aeca2fa0",
            "02667cd429ec4e60b401cdbb4e7b7585",
            "291e0bfcb3c9431a81ebf999c57505df",
            "a0a5bed710b341dfb981ad93da53452e",
            "22b7736070124da3ac81f7385c920baa",
            "f3f7c63ed8a940b9a2277b95891311bc",
            "5b6971e990bb43e6ac38c7f999e28566",
            "d3495b231d094e0cb64b3aca58b0dcad",
            "ed6be0fc9c7543f889af8be0342809b0",
            "0d41dd1deb624737a49942e92ac40b3f",
            "39a7df9d3a404320b0da2687a6d360b7",
            "953e922c96ba4b388d618558041523bd",
            "9984f56657f64688a2bfbf5b8b0d104b",
            "044e6c39bcb24726a0c6e6f898ef97de",
            "2dc807278b5c469bba1100137de370ee",
            "189f090d61494b59892ed00076a05c29",
            "35626b35957b499f836072b3b42b905c",
            "8057ce0168b340fe9cdb8bb2dd6c55c5",
            "b2fdc3f4cb764a27b984a33975c22813",
            "45c7990ad25a43ab942e7ea489ebbef0",
            "d604e869c481429298ebd2995757915a",
            "42eaf00c239445ddbc33002c20ecd01b",
            "27f9100d7edd4117a2cd41d87c118e21"
          ]
        },
        "outputId": "c746a544-5c92-451e-dc13-75b4af78d6d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Device name: NVIDIA A100-SXM4-40GB\n",
            "Device memory: 39.56427001953125 GB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0873e2d0d80c4587a6fe94a334eb6d20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/5.73M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02667cd429ec4e60b401cdbb4e7b7585"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9984f56657f64688a2bfbf5b8b0d104b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 00: 100%|██████████| 455/455 [05:28<00:00,  1.39it/s, loss=6.474]\n",
            "Processing Epoch 01: 100%|██████████| 455/455 [05:26<00:00,  1.39it/s, loss=5.989]\n",
            "Processing Epoch 02: 100%|██████████| 455/455 [05:26<00:00,  1.39it/s, loss=5.432]\n",
            "Processing Epoch 03: 100%|██████████| 455/455 [05:26<00:00,  1.39it/s, loss=5.310]\n",
            "Processing Epoch 04: 100%|██████████| 455/455 [05:26<00:00,  1.39it/s, loss=5.185]\n",
            "Processing Epoch 05: 100%|██████████| 455/455 [05:26<00:00,  1.39it/s, loss=4.964]\n"
          ]
        }
      ],
      "source": [
        "train_model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT_QNpmqzAxe",
        "outputId": "8878f03a-cf47-43dc-95e7-c6738cf4386e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n",
            "Loading weights from opus_books_trained__en_it_translation_model/encoder_decoder_model_05.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-f81f168b8be7>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(model_filename)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): EncoderStack(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x EncoderBlock(\n",
              "        (self_attention_block): MultiHeadAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-1): 2 x ResidualConnection(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (layernorm): LayerNormalization()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (decoder): DecoderStack(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x DecoderBlock(\n",
              "        (self_attention_block): MultiHeadAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (cross_attention_block): MultiHeadAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-2): 3 x ResidualConnection(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (layernorm): LayerNormalization()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (src_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(15698, 512)\n",
              "  )\n",
              "  (tgt_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(22463, 512)\n",
              "  )\n",
              "  (src_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (tgt_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (projection_layer): LinearProjectionLayer(\n",
              "    (proj): Linear(in_features=512, out_features=22463, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "def load_model(config):\n",
        "    # Get dataloaders and tokenizers\n",
        "    _, _, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "\n",
        "    # Initialize model\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size())\n",
        "\n",
        "    # Load trained weights\n",
        "    model_filename = latest_weights_file_path(config)\n",
        "    if model_filename:\n",
        "        print(f\"Loading weights from {model_filename}\")\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    return model, tokenizer_src, tokenizer_tgt, device\n",
        "\n",
        "def generate(model, tokenizer_src, tokenizer_tgt, src_text, device, max_length=350):\n",
        "    model.eval()\n",
        "\n",
        "    enc_input = tokenizer_src.encode(src_text).ids\n",
        "    enc_input = torch.tensor([tokenizer_src.token_to_id('[SOS]')] + enc_input + [tokenizer_src.token_to_id('[EOS]')]).unsqueeze(0)\n",
        "\n",
        "    enc_mask = (enc_input != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int()\n",
        "\n",
        "    enc_input = enc_input.to(device)\n",
        "    enc_mask = enc_mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_output = model.encode(enc_input, enc_mask)\n",
        "        dec_input = torch.tensor([[tokenizer_tgt.token_to_id('[SOS]')]]).to(device)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            dec_mask = causal_mask(dec_input.size(1)).to(device)\n",
        "\n",
        "            dec_output = model.decode(enc_output, enc_mask, dec_input, dec_mask)\n",
        "            proj_output = model.project(dec_output)\n",
        "\n",
        "            next_word = proj_output[:, -1].argmax(dim=-1)\n",
        "            dec_input = torch.cat([dec_input, next_word.unsqueeze(-1)], dim=1)\n",
        "\n",
        "            if next_word.item() == tokenizer_tgt.token_to_id('[EOS]'):\n",
        "                break\n",
        "\n",
        "    translated_tokens = [tokenizer_tgt.id_to_token(t.item()) for t in dec_input[0]]\n",
        "    translated_text = ' '.join([t for t in translated_tokens if t not in ['[SOS]', '[EOS]', '[PAD]']])\n",
        "\n",
        "    return translated_text\n",
        "\n",
        "\n",
        "model, tokenizer_src, tokenizer_tgt, device = load_model(config)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B1hXBPhsd4p",
        "outputId": "8e5f11ac-8867-473c-fd5d-9ba3589ea7ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English to Italian Translations:\n",
            "--------------------------------------------------\n",
            "EN: the weather is beautiful today\n",
            "IT: Il giorno è un uomo .\n",
            "--------------------------------------------------\n",
            "EN: how are you?\n",
            "IT: Come siete ?\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "test_sentences = [\n",
        "        \"the weather is beautiful today\",\n",
        "        \"how are you?\"\n",
        "    ]\n",
        "\n",
        "print(\"English to Italian Translations:\")\n",
        "print(\"-\" * 50)\n",
        "for sentence in test_sentences:\n",
        "    translation = generate(model, tokenizer_src, tokenizer_tgt, sentence, device)\n",
        "    print(f\"EN: {sentence}\")\n",
        "    print(f\"IT: {translation}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Cofqp53bQB"
      },
      "source": [
        "#### Acknowledgements\n",
        "\n",
        "This notebook is heavily adapted from a number of incredible resources on Transformers, including but not limited to:\n",
        "\n",
        "- https://blog.floydhub.com/the-transformer-in-pytorch/\n",
        "- https://arxiv.org/pdf/1706.03762.pdf\n",
        "- https://txt.cohere.com/what-are-transformer-models/\n",
        "- https://jalammar.github.io/illustrated-transformer/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0873e2d0d80c4587a6fe94a334eb6d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04dd4c00fd1a42fdae52a62f0a2fde1b",
              "IPY_MODEL_7267189d27a741b39f54c2aa8fe6f3db",
              "IPY_MODEL_7a84ac6f52444198b0a8dc06a6597cfb"
            ],
            "layout": "IPY_MODEL_4828e7291a754eafa48d7d9da724c7c8"
          }
        },
        "04dd4c00fd1a42fdae52a62f0a2fde1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6472ac80ec44a658a106ff6939179e6",
            "placeholder": "​",
            "style": "IPY_MODEL_8cd613d4fd56464c8685db4916f74233",
            "value": "README.md: 100%"
          }
        },
        "7267189d27a741b39f54c2aa8fe6f3db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1af5418e4a1e44afab78916932d647ba",
            "max": 28064,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b32028e3d5104cb0aa1dde22e2c02a4f",
            "value": 28064
          }
        },
        "7a84ac6f52444198b0a8dc06a6597cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5f12dc803a54dc8992b530bf4a05d27",
            "placeholder": "​",
            "style": "IPY_MODEL_07cb748d7e1f431b85a4c8b1aeca2fa0",
            "value": " 28.1k/28.1k [00:00&lt;00:00, 1.68MB/s]"
          }
        },
        "4828e7291a754eafa48d7d9da724c7c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6472ac80ec44a658a106ff6939179e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cd613d4fd56464c8685db4916f74233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1af5418e4a1e44afab78916932d647ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b32028e3d5104cb0aa1dde22e2c02a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5f12dc803a54dc8992b530bf4a05d27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07cb748d7e1f431b85a4c8b1aeca2fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02667cd429ec4e60b401cdbb4e7b7585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_291e0bfcb3c9431a81ebf999c57505df",
              "IPY_MODEL_a0a5bed710b341dfb981ad93da53452e",
              "IPY_MODEL_22b7736070124da3ac81f7385c920baa"
            ],
            "layout": "IPY_MODEL_f3f7c63ed8a940b9a2277b95891311bc"
          }
        },
        "291e0bfcb3c9431a81ebf999c57505df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b6971e990bb43e6ac38c7f999e28566",
            "placeholder": "​",
            "style": "IPY_MODEL_d3495b231d094e0cb64b3aca58b0dcad",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "a0a5bed710b341dfb981ad93da53452e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed6be0fc9c7543f889af8be0342809b0",
            "max": 5726189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d41dd1deb624737a49942e92ac40b3f",
            "value": 5726189
          }
        },
        "22b7736070124da3ac81f7385c920baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39a7df9d3a404320b0da2687a6d360b7",
            "placeholder": "​",
            "style": "IPY_MODEL_953e922c96ba4b388d618558041523bd",
            "value": " 5.73M/5.73M [00:00&lt;00:00, 48.1MB/s]"
          }
        },
        "f3f7c63ed8a940b9a2277b95891311bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b6971e990bb43e6ac38c7f999e28566": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3495b231d094e0cb64b3aca58b0dcad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed6be0fc9c7543f889af8be0342809b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d41dd1deb624737a49942e92ac40b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39a7df9d3a404320b0da2687a6d360b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "953e922c96ba4b388d618558041523bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9984f56657f64688a2bfbf5b8b0d104b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_044e6c39bcb24726a0c6e6f898ef97de",
              "IPY_MODEL_2dc807278b5c469bba1100137de370ee",
              "IPY_MODEL_189f090d61494b59892ed00076a05c29"
            ],
            "layout": "IPY_MODEL_35626b35957b499f836072b3b42b905c"
          }
        },
        "044e6c39bcb24726a0c6e6f898ef97de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8057ce0168b340fe9cdb8bb2dd6c55c5",
            "placeholder": "​",
            "style": "IPY_MODEL_b2fdc3f4cb764a27b984a33975c22813",
            "value": "Generating train split: 100%"
          }
        },
        "2dc807278b5c469bba1100137de370ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45c7990ad25a43ab942e7ea489ebbef0",
            "max": 32332,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d604e869c481429298ebd2995757915a",
            "value": 32332
          }
        },
        "189f090d61494b59892ed00076a05c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42eaf00c239445ddbc33002c20ecd01b",
            "placeholder": "​",
            "style": "IPY_MODEL_27f9100d7edd4117a2cd41d87c118e21",
            "value": " 32332/32332 [00:00&lt;00:00, 260062.42 examples/s]"
          }
        },
        "35626b35957b499f836072b3b42b905c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8057ce0168b340fe9cdb8bb2dd6c55c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2fdc3f4cb764a27b984a33975c22813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45c7990ad25a43ab942e7ea489ebbef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d604e869c481429298ebd2995757915a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42eaf00c239445ddbc33002c20ecd01b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27f9100d7edd4117a2cd41d87c118e21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}