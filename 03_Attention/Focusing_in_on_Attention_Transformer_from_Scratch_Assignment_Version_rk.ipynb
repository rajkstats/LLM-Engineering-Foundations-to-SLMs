{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In today's notebook, we'll be focusing on the main engine of the Transformer - ATTENTION!"
      ],
      "metadata": {
        "id": "9ZwKP3f2HGz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU torch=='2.4.1+cu121' torchvision=='0.19.1+cu121' torchaudio=='2.4.1+cu121' --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "id": "8SHpazOYCPyT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b84c7f-0ed7-4fee-bb20-f6c46b801ed3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU flash-attn"
      ],
      "metadata": {
        "id": "_TOStK75C74F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37232d6-9e79-4289-a98a-d3bdf1ce62a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/2.7 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤝 BREAKOUT ROOM #1:"
      ],
      "metadata": {
        "id": "RRKKlvApG2Oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Building Block Fundamentals of Transformer Architecture\n",
        "\n",
        "We're going to start with an example of an encoder-decoder model - the kind found in the classic paper:\n",
        "\n",
        "[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf).\n",
        "\n",
        "We'll walk through each step in code - leveraging the [PyTorch]() library heavily - in order to get an idea of how these models work.\n",
        "\n",
        "While this example notebook could be extended to a sincere usecase - we'll be using a toy dataset, and we will not fully train the model until it converges (under-train), as the full training process might take many days!"
      ],
      "metadata": {
        "id": "Z43kfJGvlQyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Desired Architecture\n",
        "\n",
        "![image](https://i.imgur.com/YPjbqW6.png)\n",
        "\n",
        "We'll skip over the diagram for now, and talk through each component in detail!"
      ],
      "metadata": {
        "id": "_sd2KYudl0Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from typing import Optional\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum"
      ],
      "metadata": {
        "id": "TPUnyvsuovuP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding\n",
        "\n",
        "![image](https://i.imgur.com/sFlEZ2e.png)\n",
        "\n",
        "The first step will be do convert our tokenized sequence of inputs into an embedding vector. This allows use to understand a rich amount of information about input sequences and their semantic meanings.\n",
        "\n",
        "As the embedding layer will be training along side the rest of the model - it will allow us to have an excellent vector-representation of the tokens in our dataset.\n",
        "\n",
        "Let's see how it looks in code!"
      ],
      "metadata": {
        "id": "JR8M3oT0l8fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int, verbose=False) -> None:\n",
        "    \"\"\"\n",
        "    vocab_size - the size of our vocabulary\n",
        "    d_model - the dimension of our embeddings and the input dimension for our model\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.verbose:\n",
        "      print(f\"Embedding Vector (1st 5 elements): {self.embedding(x)[:5] * math.sqrt(self.d_model)}\")\n",
        "    return self.embedding(x) * math.sqrt(self.d_model) # scale embeddings by square root of d_model"
      ],
      "metadata": {
        "id": "ddpQjPJWmXZg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Embedding Layer\n",
        "\n",
        "We'll set up a sample Embedding Layer and then test that it does what we'd expect!"
      ],
      "metadata": {
        "id": "ZCx8DYkaxwto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_input_embeddings_with_example():\n",
        "    # Create a small embedding layer\n",
        "    embed = InputEmbeddings(d_model=512, vocab_size=1000)\n",
        "\n",
        "    # Example sentence tokens (simplified)\n",
        "    tokens = torch.tensor([[1, 2, 3, 4, 5]])  # \"The cat sat down quickly\"\n",
        "\n",
        "    output = embed(tokens)\n",
        "    print(f\"Input shape: {tokens.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(\"\\nExample shows how words are converted to high-dimensional vectors\")\n",
        "\n",
        "    # Run technical test\n",
        "    assert output.shape == (1, 5, 512), f\"Expected shape (1, 5, 512), got {output.shape}\"\n",
        "    print(\"✓ Input Embeddings Test Passed\")"
      ],
      "metadata": {
        "id": "xl9VdXuYwfVU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_embeddings_with_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBIElPiqxORa",
        "outputId": "6d7f91fe-674b-411c-8a48-fac0cb425319"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 5])\n",
            "Output shape: torch.Size([1, 5, 512])\n",
            "\n",
            "Example shows how words are converted to high-dimensional vectors\n",
            "✓ Input Embeddings Test Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding\n",
        "\n",
        "![image](https://i.imgur.com/IIA3NK3.png)\n",
        "\n",
        "We need to impart information about where each token is in the sequence, but we aren't using any recurrence or convolutions - the easiest way to encode positional information is to inject positional information into our input embeddings.\n",
        "\n",
        "We're going to use the process outlined in the paper to do this - which is to use a specific combination of functions to add positional information to the embeddings."
      ],
      "metadata": {
        "id": "ra5KCa1KnfrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, seq_len: int, dropout: float, verbose=False) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.verbose=verbose\n",
        "\n",
        "    positional_embeddings = torch.zeros(seq_len, d_model)\n",
        "    positional_sequence_vector = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    positional_model_vector = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    positional_embeddings[:, 0::2] = torch.sin(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings[:, 1::2] = torch.cos(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings = positional_embeddings.unsqueeze(0)\n",
        "\n",
        "    self.register_buffer('positional_embeddings', positional_embeddings)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.positional_embeddings[:, :x.shape[1], :]).requires_grad_(False)\n",
        "    if self.verbose:\n",
        "      print(f\"Positional Encodings (1st 5 elements): {x}\")\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "5K3NXh7MoM5D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Positional Encoding Layer\n",
        "\n",
        "We'll set up a sample Positional Encoding Layer and then test that it does what we'd expect!"
      ],
      "metadata": {
        "id": "zkcMmoT2yOZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_positional_encoding_with_example():\n",
        "    pos = PositionalEncoding(d_model=512, seq_len=10, dropout=0.1)\n",
        "\n",
        "    # Create sample embeddings for \"The cat sat\"\n",
        "    x = torch.randn(1, 3, 512)\n",
        "\n",
        "    output = pos(x)\n",
        "    print(\"Input tokens position:  [1, 2, 3]\")\n",
        "    print(\"Added position info to each word's embedding\")\n",
        "    print(f\"Output maintains shape: {output.shape}\")\n",
        "\n",
        "    # Verify position information was added\n",
        "    assert not torch.allclose(output, x), \"Position information should modify embeddings\"\n",
        "    print(\"✓ Positional Encoding Test Passed\")"
      ],
      "metadata": {
        "id": "ayoWMbSYySBp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_positional_encoding_with_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4hDXJaGyU5M",
        "outputId": "3312b7bb-85bf-48ae-b3a1-60b1fcee6da7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokens position:  [1, 2, 3]\n",
            "Added position info to each word's embedding\n",
            "Output maintains shape: torch.Size([1, 3, 512])\n",
            "✓ Positional Encoding Test Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add & Norm\n",
        "\n",
        "Next we'll tackle the Add & Norm Block of the diagram.\n",
        "\n",
        "![image](https://i.imgur.com/otdEq4D.png)"
      ],
      "metadata": {
        "id": "ueiM7LzKpcFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer Normalization\n",
        "\n",
        "The first step is to add layer normalization. You can read more about it [here](https://paperswithcode.com/method/layer-normalization)!\n",
        "\n",
        "The basic idea is that it makes training the model a bit easier, and allows the model to generalize a bit better."
      ],
      "metadata": {
        "id": "_lDABPLSKqOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, features: int, epsilon:float=10**-6) -> None:\n",
        "    super().__init__()\n",
        "    self.epsilon = epsilon\n",
        "    self.gamma = nn.Parameter(torch.ones(features))\n",
        "    self.beta = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim = -1, keepdim = True)\n",
        "    standard_deviation = x.std(dim = -1, keepdim = True)\n",
        "    return self.gamma * (x - mean) / (standard_deviation + self.epsilon) + self.beta"
      ],
      "metadata": {
        "id": "1Nlv7BH7ruSf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Layer Normalization\n",
        "\n",
        "We'll set up a sample Layer Normalization and then test that it does what we'd expect!"
      ],
      "metadata": {
        "id": "vft5kFUBy1aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_layer_normalization_with_example():\n",
        "    layer_norm = LayerNormalization(features=3)  # Smaller feature size for example\n",
        "\n",
        "    # Simulate word embeddings with different magnitudes\n",
        "    word_embeddings = torch.tensor([\n",
        "        [2.5, 4.1, -3.2],  # \"The\" (high magnitude)\n",
        "        [0.1, 0.2, -0.1],  # \"cat\" (low magnitude)\n",
        "        [8.2, -6.1, 5.5]   # \"sat\" (very high magnitude)\n",
        "    ]).unsqueeze(0)\n",
        "\n",
        "    normalized = layer_norm(word_embeddings)\n",
        "\n",
        "    print(\"Before normalization (magnitudes vary greatly):\")\n",
        "    print(word_embeddings[0])\n",
        "    print(\"\\nAfter normalization (values scaled to similar ranges):\")\n",
        "    print(normalized[0])\n",
        "\n",
        "    # Verify statistical properties\n",
        "    mean = normalized.mean(dim=-1)\n",
        "    var = normalized.var(dim=-1)\n",
        "    assert torch.allclose(mean, torch.zeros_like(mean), atol=1e-5)\n",
        "    assert torch.allclose(var, torch.ones_like(var), atol=1e-5)\n",
        "    print(\"✓ Layer Normalization Test Passed\")"
      ],
      "metadata": {
        "id": "01Llr9cwy7C2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_layer_normalization_with_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75hToLCry9mR",
        "outputId": "ec3805f4-4d14-4867-d367-12394708a4ef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before normalization (magnitudes vary greatly):\n",
            "tensor([[ 2.5000,  4.1000, -3.2000],\n",
            "        [ 0.1000,  0.2000, -0.1000],\n",
            "        [ 8.2000, -6.1000,  5.5000]])\n",
            "\n",
            "After normalization (values scaled to similar ranges):\n",
            "tensor([[ 0.3562,  0.7732, -1.1293],\n",
            "        [ 0.2182,  0.8729, -1.0911],\n",
            "        [ 0.7459, -1.1363,  0.3905]], grad_fn=<SelectBackward0>)\n",
            "✓ Layer Normalization Test Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residual Connection\n",
        "\n",
        "Another technique that makes model training easier, we add a Residual connection to the outputs of the Attention Block - this helps to prevent vanishing gradient."
      ],
      "metadata": {
        "id": "rwXt7KKKycrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, features: int, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.layernorm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x + self.dropout(sublayer(self.layernorm(x)))"
      ],
      "metadata": {
        "id": "XKwGFD2-yd-Z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Residual Connection\n",
        "\n",
        "We'll set up a sample Residual Connection and then test that it does what we'd expect!"
      ],
      "metadata": {
        "id": "p668YYgBzc3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_residual_connection_with_example():\n",
        "    residual = ResidualConnection(features=3, dropout=0.1)\n",
        "\n",
        "    # Original input \"The cat\"\n",
        "    x = torch.tensor([\n",
        "        [1.0, 1.0, 1.0],\n",
        "        [2.0, 2.0, 2.0]\n",
        "    ]).unsqueeze(0)\n",
        "\n",
        "    # Sublayer that makes meaningful changes\n",
        "    def sublayer(x):\n",
        "        return torch.nn.functional.relu(x + 0.5) # Non-linear transformation\n",
        "\n",
        "    output = residual(x, sublayer)\n",
        "\n",
        "    print(\"Original input:\")\n",
        "    print(x[0])\n",
        "    print(\"\\nAfter residual connection (combines original + transformed):\")\n",
        "    print(output[0])\n",
        "\n",
        "    # Verify output changed but maintained shape\n",
        "    assert output.shape == x.shape\n",
        "    assert torch.any(torch.abs(output - x) > 1e-6), \"Output should differ from input\"\n",
        "    print(\"✓ Residual Connection Test Passed\")"
      ],
      "metadata": {
        "id": "heebV5mIzje4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_residual_connection_with_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZMF65cmzmgx",
        "outputId": "e2cee609-a638-41b0-b342-b694474ea8ec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original input:\n",
            "tensor([[1., 1., 1.],\n",
            "        [2., 2., 2.]])\n",
            "\n",
            "After residual connection (combines original + transformed):\n",
            "tensor([[1.5556, 1.5556, 1.5556],\n",
            "        [2.5556, 2.5556, 2.5556]], grad_fn=<SelectBackward0>)\n",
            "✓ Residual Connection Test Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed Forward Network\n",
        "\n",
        "![image](https://i.imgur.com/woEqBjQ.png)\n",
        "\n",
        "Moving onto the next component, we have our feed forward network.\n",
        "\n",
        "The feed forward networks servers two purposes in our model:\n",
        "\n",
        "1. It reforms the attention outputs into a format that works with the next block.\n",
        "\n",
        "2. It helps add complexity to prevent each attention block acting in a similar fashion."
      ],
      "metadata": {
        "id": "IIOZp3xhsaXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1) -> None:\n",
        "    \"\"\"\n",
        "    d_model - dimension of model\n",
        "    d_ff - dimension of feed forward network\n",
        "    dropout - regularization measure\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ],
      "metadata": {
        "id": "JueNG2UBszbR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Feed-forward Block\n",
        "\n",
        "Let's test!"
      ],
      "metadata": {
        "id": "cU-H1m4L0UhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_feed_forward_block_with_example():\n",
        "   ff_block = FeedForwardBlock(d_model=3, d_ff=8)  # Small dimensions for demonstration\n",
        "\n",
        "   # Input: Word embeddings for \"The cat\"\n",
        "   x = torch.tensor([\n",
        "       [1.0, 0.5, 0.2],  # \"The\"\n",
        "       [2.0, -0.3, 1.1]  # \"cat\"\n",
        "   ]).unsqueeze(0)\n",
        "\n",
        "   output = ff_block(x)\n",
        "\n",
        "   print(\"Input embeddings:\")\n",
        "   print(x[0])\n",
        "   print(\"\\nAfter feed-forward transformation:\")\n",
        "   print(output[0])\n",
        "\n",
        "   # First linear layer expands to d_ff dimensions\n",
        "   # ReLU keeps only positive values\n",
        "   # Second linear layer projects back to d_model dimensions\n",
        "   assert output.shape == x.shape\n",
        "   assert not torch.allclose(output, x)\n",
        "   print(\"✓ Feed Forward Block Test Passed\")"
      ],
      "metadata": {
        "id": "pwtbsJou0fxz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_feed_forward_block_with_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMHE9fWB0hj3",
        "outputId": "985db144-3491-45d3-c9ce-07dc7b9751f8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input embeddings:\n",
            "tensor([[ 1.0000,  0.5000,  0.2000],\n",
            "        [ 2.0000, -0.3000,  1.1000]])\n",
            "\n",
            "After feed-forward transformation:\n",
            "tensor([[ 0.4980, -0.4212,  0.1170],\n",
            "        [ 0.3088, -0.1387,  0.2029]], grad_fn=<SelectBackward0>)\n",
            "✓ Feed Forward Block Test Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task #1: Multi-Head Attention\n",
        "\n",
        "![image](https://i.imgur.com/4qOT46y.png)\n",
        "\n",
        "Next up is the heart and soul of the Transformer - Multi-Head Attention.\n",
        "\n",
        "We'll break it down into the basic building blocks in code in the following section!"
      ],
      "metadata": {
        "id": "bJ5EGkvdtP0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Multi-Head Attention Class\n",
        "\n"
      ],
      "metadata": {
        "id": "-Oto52ZyvX0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    assert d_model % num_heads == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)"
      ],
      "metadata": {
        "id": "mDHkw1f_vmuv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 👪❓Discussion Question 1:\n",
        "\n",
        "Describe, in your own words, intuitively what each of `Q`, `K`, and `V` represent - beyond their names and functions.\n",
        "\n",
        "Discuss among your group!\n",
        "\n",
        "There is no single correct answer - and if you get stuck - feel free to ask your favourite Large Language Model!\n",
        "\n",
        "\n",
        "### ANSWER\n",
        "Think of online dating apps\n",
        "- **Query:Q** is your dating preferences and what you're looking for in a partner. Like \"I want someone who loves hiking and cooking\".\n",
        "- **Keys:K** are like the tags or highlights in other people's dating profiles - their interests, hobbies, and characteristics that you can match against.\n",
        "- **Values:V** are the actual people and their full profiles - the meaningful content you get to interact with once you match.\n",
        "\n",
        "The attention mechanism works just like the dating app's matching system:\n",
        "\n",
        "- Your preferences (Q) are compared against everyone's profile highlights (K)\n",
        "- Profiles with matching interests get higher \"compatibility scores\"\n",
        "- You then get to see and interact with the most compatible matches (V) more than others\n",
        "\n",
        "Just like how you might give more \"attention\" to profiles that match your interests, the attention mechanism weighs different Values based on how well their keys match your query"
      ],
      "metadata": {
        "id": "dEm7I6iwMiom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Multi-Head Attention\n",
        "\n",
        "Let's test it out!"
      ],
      "metadata": {
        "id": "9dp3eM7H1cRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task #3: Scaled Dot-Product Attention\n",
        "\n",
        "![image](https://i.imgur.com/Yp48DuB.png)"
      ],
      "metadata": {
        "id": "FyD7nAM6tb0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value, mask, d_k, dropout: nn.Dropout = None):\n",
        "  attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "    attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "  attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "  if dropout is not None:\n",
        "    attention_scores = dropout(attention_scores)\n",
        "\n",
        "  return (attention_scores @ value), attention_scores"
      ],
      "metadata": {
        "id": "dKk08SvowJIc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ❓Question 1:\n",
        "\n",
        "Describe the above code that defines the attension mechanism.\n",
        "\n",
        "Write, in natural language, what each step is doing.\n",
        "\n",
        "### ANSWER\n",
        "**DOT PRODUCT**\n",
        "- Performs dot product between query and key matrices\n",
        "- Scaling by √d_k prevents dot products from getting too large\n",
        "- Result shows how much each word should attend to other words\n",
        "\n",
        "**Masking**\n",
        "- Applies mask to prevent attention to certain positions\n",
        "- Sets masked positions to very negative number (-1e9)\n",
        "- After softmax, masked positions will be close to 0\n",
        "\n",
        "**Transform scores**\n",
        "- Converts scores to probabilities between 0 and 1\n",
        "-  Each row sums to 1\n",
        "- Higher scores get higher probabilities\n",
        "\n",
        "**Dropout:Prevent overfitting**\n",
        "- Dropout: Randomly zeros out some attention scores and helps prevent overfitting\n",
        "- Multiplies attention probabilities with values\n",
        "- Returns both output and attention scores"
      ],
      "metadata": {
        "id": "qphuqoXl-1jW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task #4: Forward Method\n",
        "\n",
        "This is code is required to do a forward pass with our model."
      ],
      "metadata": {
        "id": "Ac8k7b3SxKOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, query, key, value, mask):\n",
        "  query = self.w_q(query)\n",
        "  key = self.w_k(key)\n",
        "  value = self.w_v(value)\n",
        "\n",
        "  query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "  key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "  value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "  x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "  x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "  return self.w_o(x)"
      ],
      "metadata": {
        "id": "um2Oxv1axWYK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task #4: Combining it All Together - and Introducing Flash Attention\n",
        "\n",
        "We'll start with implementing a simple set of `configs` - and then get straight into an Attention implementation!"
      ],
      "metadata": {
        "id": "kgr_D0mmyEgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionType(Enum):\n",
        "    REGULAR = \"regular\"\n",
        "    FLASH = \"flash\"\n",
        "\n",
        "@dataclass\n",
        "class AttentionConfig:\n",
        "    attention_type: AttentionType\n",
        "    d_model: int = 512\n",
        "    num_heads: int = 8\n",
        "    dropout: float = 0.1"
      ],
      "metadata": {
        "id": "E-AMu8nkGQM2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All we need to do is tie together the above steps into one big block!"
      ],
      "metadata": {
        "id": "WPOFAp7lIb0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModularAttention(nn.Module):\n",
        "    def __init__(self, config: AttentionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.d_model = config.d_model\n",
        "        self.num_heads = config.num_heads\n",
        "        self.d_k = config.d_model // config.num_heads\n",
        "\n",
        "        self.w_q = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.w_k = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.w_v = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.w_o = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "    def _regular_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor,\n",
        "                         mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
        "        d_k = q.shape[-1]\n",
        "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        if self.training:\n",
        "            attention_weights = torch.dropout(attention_weights, self.dropout, self.training)\n",
        "\n",
        "        return attention_weights @ v\n",
        "\n",
        "    def _flash_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor,\n",
        "                    mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
        "        try:\n",
        "            from flash_attn import flash_attn_func\n",
        "\n",
        "            # Verify inputs are in the correct format\n",
        "            if not all(x.dtype == torch.float16 for x in [q, k, v]):\n",
        "                print(\"Warning: Inputs must be in float16 for Flash Attention\")\n",
        "                return self._regular_attention(q, k, v, mask)\n",
        "\n",
        "            if not all(x.is_cuda for x in [q, k, v]):\n",
        "                print(\"Warning: Inputs must be on CUDA for Flash Attention\")\n",
        "                return self._regular_attention(q, k, v, mask)\n",
        "\n",
        "            # Convert boolean mask to float mask where 0.0 means masked\n",
        "            if mask is not None:\n",
        "                mask = mask.to(dtype=torch.float32)\n",
        "\n",
        "            dropout_p = self.dropout if self.training else 0.0\n",
        "\n",
        "            # Make tensors contiguous\n",
        "            q = q.contiguous()\n",
        "            k = k.contiguous()\n",
        "            v = v.contiguous()\n",
        "\n",
        "            # Call flash attention\n",
        "            return flash_attn_func(\n",
        "                q, k, v,\n",
        "                dropout_p=dropout_p,\n",
        "                causal=False,\n",
        "                softmax_scale=None\n",
        "            )\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"Flash Attention not available, falling back to regular attention\")\n",
        "            return self._regular_attention(q, k, v, mask)\n",
        "        except Exception as e:\n",
        "            print(f\"Error using Flash Attention: {str(e)}\")\n",
        "            print(\"Falling back to regular attention\")\n",
        "            return self._regular_attention(q, k, v, mask)\n",
        "\n",
        "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
        "                mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # Linear projections and reshape\n",
        "        q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Make contiguous for Flash Attention\n",
        "        q, k, v = q.contiguous(), k.contiguous(), v.contiguous()\n",
        "\n",
        "        # Choose attention implementation\n",
        "        if self.config.attention_type == AttentionType.FLASH:\n",
        "            attn_output = self._flash_attention(q, k, v, mask)\n",
        "        else:\n",
        "            attn_output = self._regular_attention(q, k, v, mask)\n",
        "\n",
        "        # Reshape and project back\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.w_o(attn_output)"
      ],
      "metadata": {
        "id": "QKiXahC_GKMV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👪❓Discussion Question 2:\n",
        "\n",
        "Work with your breakout room to build a visualization of the above `forward` pass - including dummy inputs.\n",
        "\n",
        "You can use drawing programs (like [Excalidraw](https://excalidraw.com/)), or write a visualization in code.\n",
        "\n",
        "> NOTE: LLMs like Claude 3.5 Sonnet is a fantastic tool to produce (and test) visualizations in it's [Web UI](https://claude.ai/new)!"
      ],
      "metadata": {
        "id": "ySBbU7Zo_x5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing MultiHeadAttention\n",
        "\n",
        "Let's test it out!"
      ],
      "metadata": {
        "id": "qRFqoZyD1-AU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_attention_mechanisms():\n",
        "    attention_configs = [\n",
        "        AttentionConfig(attention_type=AttentionType.REGULAR, d_model=6, num_heads=2, dropout=0.1),\n",
        "        AttentionConfig(attention_type=AttentionType.FLASH, d_model=6, num_heads=2, dropout=0.1)\n",
        "    ]\n",
        "\n",
        "    # Create sequence in float16\n",
        "    seq = torch.tensor([\n",
        "        [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
        "        [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],\n",
        "        [0.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n",
        "    ], dtype=torch.float16).unsqueeze(0).cuda()\n",
        "\n",
        "    mask = None\n",
        "\n",
        "    for config in attention_configs:\n",
        "        try:\n",
        "            print(f\"\\nTesting {config.attention_type.value} attention:\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            mha = ModularAttention(config).cuda().half()\n",
        "\n",
        "            # Convert model weights to float16\n",
        "            for param in mha.parameters():\n",
        "                param.data = param.data.half()\n",
        "\n",
        "            output = mha(seq, seq, seq, mask)\n",
        "\n",
        "            print(f\"Input shape: {seq.shape}\")\n",
        "            print(\"Input values:\")\n",
        "            print(seq[0])\n",
        "            print(\"\\nOutput values:\")\n",
        "            print(output[0])\n",
        "\n",
        "            assert output.shape == seq.shape\n",
        "            assert not torch.allclose(output, seq)\n",
        "            print(f\"✓ {config.attention_type.value.title()} Attention Test Passed\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(f\"Flash Attention not available, skipping {config.attention_type.value} test\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error testing {config.attention_type.value} attention: {str(e)}\")\n",
        "\n",
        "        # Cleanup to prevent CUDA memory issues\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "sY1N60di2CHQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_attention_mechanisms()"
      ],
      "metadata": {
        "id": "2qEqn4BS2DHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a8986a3-0149-4e0f-95fa-6ff8c1775436"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing regular attention:\n",
            "--------------------------------------------------\n",
            "Input shape: torch.Size([1, 3, 6])\n",
            "Input values:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 1.]], device='cuda:0', dtype=torch.float16)\n",
            "\n",
            "Output values:\n",
            "tensor([[-0.1061,  0.0992, -0.0269, -0.1022, -0.1265, -0.0900],\n",
            "        [-0.0251,  0.0383,  0.0085, -0.1440, -0.1050,  0.0126],\n",
            "        [-0.0878,  0.1097, -0.0047, -0.1261, -0.1235, -0.0649]],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "✓ Regular Attention Test Passed\n",
            "\n",
            "Testing flash attention:\n",
            "--------------------------------------------------\n",
            "Input shape: torch.Size([1, 3, 6])\n",
            "Input values:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 1.]], device='cuda:0', dtype=torch.float16)\n",
            "\n",
            "Output values:\n",
            "tensor([[ 0.0764,  0.2139, -0.1443, -0.0685, -0.1674, -0.2524],\n",
            "        [-0.0842, -0.0127,  0.1257,  0.0593,  0.1155,  0.0158],\n",
            "        [-0.0725, -0.0012,  0.3413,  0.0076, -0.1886, -0.0633]],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "✓ Flash Attention Test Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 features: int,\n",
        "                 attn_config: AttentionConfig,\n",
        "                 feed_forward: nn.Module,\n",
        "                 dropout: float,\n",
        "                 is_decoder: bool = False):\n",
        "        super().__init__()\n",
        "        self.is_decoder = is_decoder\n",
        "        self.self_attention = ModularAttention(attn_config)\n",
        "        self.feed_forward = feed_forward\n",
        "\n",
        "        num_connections = 3 if is_decoder else 2\n",
        "        self.residual_connections = nn.ModuleList([\n",
        "            ResidualConnection(features, dropout) for _ in range(num_connections)\n",
        "        ])\n",
        "\n",
        "        if is_decoder:\n",
        "            self.cross_attention = ModularAttention(attn_config)\n",
        "\n",
        "    def forward(self, x: torch.Tensor,\n",
        "                encoder_output: Optional[torch.Tensor] = None,\n",
        "                self_mask: Optional[torch.Tensor] = None,\n",
        "                cross_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention(x, x, x, self_mask))\n",
        "\n",
        "        if self.is_decoder:\n",
        "            if encoder_output is None:\n",
        "                raise ValueError(\"Decoder block requires encoder_output\")\n",
        "            x = self.residual_connections[1](x, lambda x: self.cross_attention(x, encoder_output, encoder_output, cross_mask))\n",
        "            return self.residual_connections[2](x, self.feed_forward)\n",
        "\n",
        "        return self.residual_connections[1](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "vLJ3xAJcIizf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder\n",
        "\n",
        "When we pass information through our model - the first thing we will do is Encode it by passing it through our Encoder Blocks.\n"
      ],
      "metadata": {
        "id": "EkKjLhpiyz5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Block\n",
        "\n",
        "![image](https://i.imgur.com/nwNYZAT.png)\n",
        "\n",
        "The encoder takes in the source language sentence (e.g. English). Each word is converted into a vector representation using an embedding layer. Then a positional encoder adds information about the position of each word. This goes through multiple self-attention layers, where each word vector attends to all other word vectors to build contextual representations."
      ],
      "metadata": {
        "id": "f-0edIfMzijj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, features: int, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, input_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, input_mask))\n",
        "    x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "    return x"
      ],
      "metadata": {
        "id": "dMVnZiGDy1PG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the EncoderBlock\n",
        "\n",
        "Testing time!"
      ],
      "metadata": {
        "id": "_twqqReS28ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_encoder_blocks():\n",
        "    # Use float32 for regular attention, float16 only for flash attention\n",
        "    input_seq = torch.tensor([\n",
        "        [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"The\"\n",
        "        [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
        "        [0.0, 0.0, 0.0, 0.0, 1.0, 1.0]   # \"sleeps\"\n",
        "    ]).unsqueeze(0).cuda()\n",
        "\n",
        "    mask = torch.ones(1, 1, 3, 3, dtype=torch.bool).cuda()\n",
        "\n",
        "    attention_configs = [\n",
        "        AttentionConfig(AttentionType.REGULAR, d_model=6, num_heads=2, dropout=0.1),\n",
        "        AttentionConfig(AttentionType.FLASH, d_model=6, num_heads=2, dropout=0.1)\n",
        "    ]\n",
        "\n",
        "    for config in attention_configs:\n",
        "        try:\n",
        "            print(f\"\\nTesting Encoder with {config.attention_type.value} attention:\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            attention = ModularAttention(config).cuda()\n",
        "            ff = FeedForwardBlock(d_model=6, d_ff=12).cuda()\n",
        "            encoder = TransformerBlock(\n",
        "                features=6,\n",
        "                attn_config=config,\n",
        "                feed_forward=ff,\n",
        "                dropout=0.1\n",
        "            ).cuda()\n",
        "\n",
        "            # Convert to float16 only for flash attention\n",
        "            if config.attention_type == AttentionType.FLASH:\n",
        "                input_seq = input_seq.half()\n",
        "                encoder = encoder.half()\n",
        "                for param in encoder.parameters():\n",
        "                    param.data = param.data.half()\n",
        "\n",
        "            output = encoder(input_seq, self_mask=mask)\n",
        "\n",
        "            print(\"Input sequence:\")\n",
        "            print(input_seq[0])\n",
        "            print(\"\\nAfter encoder processing:\")\n",
        "            print(output[0])\n",
        "\n",
        "            assert output.shape == input_seq.shape\n",
        "            assert not torch.allclose(output, input_seq)\n",
        "            print(f\"✓ Encoder Block with {config.attention_type.value} attention Test Passed\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(f\"Flash Attention not available, skipping {config.attention_type.value} test\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error testing encoder with {config.attention_type.value} attention: {str(e)}\")\n",
        "\n",
        "        # Reset to float32 for next iteration\n",
        "        input_seq = input_seq.float()\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "PP_wq0R02_g0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encoder_blocks()"
      ],
      "metadata": {
        "id": "DPAH-NKA3AEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91c40eec-00ff-46fe-b1be-c72c09103616"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Encoder with regular attention:\n",
            "--------------------------------------------------\n",
            "Input sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 1.]], device='cuda:0')\n",
            "\n",
            "After encoder processing:\n",
            "tensor([[ 0.8157,  1.2143, -0.1109,  0.2407, -0.1842, -0.2401],\n",
            "        [-0.4664,  0.1175,  1.1868,  0.8676, -0.4599,  0.0434],\n",
            "        [-0.2773,  0.2382,  0.3886, -0.2353,  1.1140,  0.8515]],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "✓ Encoder Block with regular attention Test Passed\n",
            "\n",
            "Testing Encoder with flash attention:\n",
            "--------------------------------------------------\n",
            "Input sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 1.]], device='cuda:0', dtype=torch.float16)\n",
            "\n",
            "After encoder processing:\n",
            "tensor([[ 1.0293,  0.8843,  0.0778,  0.1447, -0.3958, -0.0247],\n",
            "        [ 0.6655, -0.3599,  0.6362,  1.3574,  0.5137,  0.2054],\n",
            "        [-0.1438,  0.4443,  0.9854, -0.6182, -0.0850,  1.3428]],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "✓ Encoder Block with flash attention Test Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Stack\n",
        "\n",
        "Following along from the original paper - we will organize these blocks into a set of 6.\n",
        "\n",
        "These 6 Encoder Blocks (each with 8 Attention Heads) will comprise our Encoding Stack."
      ],
      "metadata": {
        "id": "a-AvnoPrzvwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderStack(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "kOaR5SjUzxv7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder\n",
        "\n",
        "Next, we will take the encoded sequence and decode it through our Decoder Blocks."
      ],
      "metadata": {
        "id": "fQyujsRTz5NT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Block\n",
        "\n",
        "![image](https://i.imgur.com/HtAAXZc.png)\n",
        "\n",
        "The decoder takes in the target language sentence (e.g. Italian). It also converts words to vectors and adds positional info. Then it goes through self-attention layers. Here, a mask is applied so each word can only see the words before it, not after.\n",
        "\n",
        "The decoder also does attention over the encoder output. This allows each French word to find relevant connections with the English words."
      ],
      "metadata": {
        "id": "zBYgl77Kz6bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, features: int, self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "  def forward(self, x, encoder_output, input_mask, target_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, target_mask))\n",
        "    x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, input_mask))\n",
        "    x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "    return x"
      ],
      "metadata": {
        "id": "SIwafbqzz5-n"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing DecoderBlock\n",
        "\n",
        "You know what's up next...testing!"
      ],
      "metadata": {
        "id": "s0KkwjkM3xe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_decoder_blocks():\n",
        "    x = torch.tensor([\n",
        "        [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"El\"\n",
        "        [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"gato\"\n",
        "    ]).unsqueeze(0).cuda()\n",
        "\n",
        "    encoder_output = torch.tensor([\n",
        "        [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],  # \"The\"\n",
        "        [0.0, 0.0, 1.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
        "    ]).unsqueeze(0).cuda()\n",
        "\n",
        "    cross_mask = torch.ones(1, 1, 2, 2, dtype=torch.bool).cuda()\n",
        "    self_mask = torch.tril(torch.ones(1, 1, 2, 2, dtype=torch.bool)).cuda()\n",
        "\n",
        "    attention_configs = [\n",
        "        AttentionConfig(AttentionType.REGULAR, d_model=6, num_heads=2, dropout=0.1),\n",
        "        AttentionConfig(AttentionType.FLASH, d_model=6, num_heads=2, dropout=0.1)\n",
        "    ]\n",
        "\n",
        "    for config in attention_configs:\n",
        "        try:\n",
        "            print(f\"\\nTesting Decoder with {config.attention_type.value} attention:\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            ff = FeedForwardBlock(d_model=6, d_ff=12).cuda()\n",
        "            decoder = TransformerBlock(\n",
        "                features=6,\n",
        "                attn_config=config,\n",
        "                feed_forward=ff,\n",
        "                dropout=0.1,\n",
        "                is_decoder=True\n",
        "            ).cuda()\n",
        "\n",
        "            if config.attention_type == AttentionType.FLASH:\n",
        "                x = x.half()\n",
        "                encoder_output = encoder_output.half()\n",
        "                decoder = decoder.half()\n",
        "                for param in decoder.parameters():\n",
        "                    param.data = param.data.half()\n",
        "\n",
        "            output = decoder(x, encoder_output=encoder_output,\n",
        "                           self_mask=self_mask, cross_mask=cross_mask)\n",
        "\n",
        "            print(\"Input target sequence:\")\n",
        "            print(x[0])\n",
        "            print(\"\\nSource (encoder) sequence:\")\n",
        "            print(encoder_output[0])\n",
        "            print(\"\\nDecoder output:\")\n",
        "            print(output[0])\n",
        "\n",
        "            assert output.shape == x.shape\n",
        "            assert not torch.allclose(output, x)\n",
        "            print(f\"✓ Decoder Block with {config.attention_type.value} attention Test Passed\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(f\"Flash Attention not available, skipping {config.attention_type.value} test\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error testing decoder with {config.attention_type.value} attention: {str(e)}\")\n",
        "\n",
        "        x = x.float()\n",
        "        encoder_output = encoder_output.float()\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "UnmnWAVl307S"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_decoder_blocks()"
      ],
      "metadata": {
        "id": "COJlCq3033A2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f3dd40-f80d-4af4-f0b3-a698b5541d62"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Decoder with regular attention:\n",
            "--------------------------------------------------\n",
            "Input target sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.]], device='cuda:0')\n",
            "\n",
            "Source (encoder) sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.]], device='cuda:0')\n",
            "\n",
            "Decoder output:\n",
            "tensor([[ 1.6909,  1.8783,  1.0077,  0.0926,  0.8426, -0.0555],\n",
            "        [ 0.5649,  0.5202,  0.9035,  0.9005,  0.5575, -0.3772]],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "✓ Decoder Block with regular attention Test Passed\n",
            "\n",
            "Testing Decoder with flash attention:\n",
            "--------------------------------------------------\n",
            "Input target sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
            "\n",
            "Source (encoder) sequence:\n",
            "tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
            "\n",
            "Decoder output:\n",
            "tensor([[ 1.3789,  1.2305,  0.0718,  0.3767, -0.4966,  0.4507],\n",
            "        [-0.0784,  0.1708,  1.6133,  1.1982, -0.5869, -0.0489]],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "✓ Decoder Block with flash attention Test Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Stack\n",
        "\n",
        "We'll use the same number of Decoder Blocks as we did Encoder Blocks - leaving us with 6 Deocder Blocks in our Decoder Stack."
      ],
      "metadata": {
        "id": "sa4yiTNn0BkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderStack(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, encoder_output, input_mask, target_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, input_mask, target_mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "1QUXzOXk0CcT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Projection Layer\n",
        "\n",
        "After the decoder's self-attention and encoder-decoder attention layers, we have a context vector representing each Italian word. This context vector has a high dimension (e.g. 512 or 1024).\n",
        "\n",
        "We want to take this context vector and generate a probability distribution over the French vocabulary so we can pick the next translated word.\n",
        "\n",
        "The linear projection layer helps with this. It projects the context vector into a much larger vector called the vocabulary distribution - one entry per word in the vocabulary.\n",
        "\n",
        "For example, if our Italian vocabulary has 50,000 words, the vocabulary distribution will have 50,000 dimensions. Each dimension corresponds to the probability of that Italian word being the correct translation."
      ],
      "metadata": {
        "id": "kRFiAP580S4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearProjectionLayer(nn.Module):\n",
        "  def __init__(self, d_model, vocab_size) -> None:\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x) -> None:\n",
        "    return self.proj(x)"
      ],
      "metadata": {
        "id": "tkBBMAZK0WLB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Transformer\n",
        "\n",
        "At this point, all we need to do is create a class that represents our model!"
      ],
      "metadata": {
        "id": "v9ucsRWs0lG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder: EncoderBlock, decoder: DecoderBlock, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: LinearProjectionLayer) -> None:\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.src_pos = src_pos\n",
        "    self.tgt_pos = tgt_pos\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    src = self.src_embed(src)\n",
        "    src = self.src_pos(src)\n",
        "    return self.encoder(src, src_mask)\n",
        "\n",
        "  def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "    tgt = self.tgt_embed(tgt)\n",
        "    tgt = self.tgt_pos(tgt)\n",
        "    return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "  def project(self, x):\n",
        "    return self.projection_layer(x)"
      ],
      "metadata": {
        "id": "5ip11mmQ0nMM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Our Transformer\n",
        "\n",
        "Now that we have each of our components - we need to construct an actual model!\n",
        "\n",
        "We'll use this helper function to aid in our goal and set up our Encoder/Decoder Stacks!"
      ],
      "metadata": {
        "id": "4Ssr5nA039--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(config: dict, attention_type: AttentionType = AttentionType.REGULAR) -> nn.Module:\n",
        "    attn_config = AttentionConfig(\n",
        "        attention_type=attention_type,\n",
        "        d_model=config['d_model'],\n",
        "        num_heads=config.get('num_heads', 8),\n",
        "        dropout=config.get('dropout', 0.1)\n",
        "    )\n",
        "\n",
        "    input_embeddings = InputEmbeddings(config['d_model'], config['input_vocab_size'])\n",
        "    target_embeddings = InputEmbeddings(config['d_model'], config['target_vocab_size'])\n",
        "\n",
        "    input_position = PositionalEncoding(config['d_model'], config['seq_len'], config.get('dropout', 0.1))\n",
        "    target_position = PositionalEncoding(config['d_model'], config['seq_len'], config.get('dropout', 0.1))\n",
        "\n",
        "    encoder_blocks = [\n",
        "        TransformerBlock(\n",
        "            features=config['d_model'],\n",
        "            attn_config=attn_config,\n",
        "            feed_forward=FeedForwardBlock(config['d_model'], config.get('d_ff', 2048)),\n",
        "            dropout=config.get('dropout', 0.1)\n",
        "        ) for _ in range(config.get('N', 6))\n",
        "    ]\n",
        "\n",
        "    decoder_blocks = [\n",
        "        TransformerBlock(\n",
        "            features=config['d_model'],\n",
        "            attn_config=attn_config,\n",
        "            feed_forward=FeedForwardBlock(config['d_model'], config.get('d_ff', 2048)),\n",
        "            dropout=config.get('dropout', 0.1),\n",
        "            is_decoder=True\n",
        "        ) for _ in range(config.get('N', 6))\n",
        "    ]\n",
        "\n",
        "    model = Transformer(\n",
        "        encoder=EncoderStack(config['d_model'], nn.ModuleList(encoder_blocks)),\n",
        "        decoder=DecoderStack(config['d_model'], nn.ModuleList(decoder_blocks)),\n",
        "        src_embed=input_embeddings,\n",
        "        tgt_embed=target_embeddings,\n",
        "        src_pos=input_position,\n",
        "        tgt_pos=target_position,\n",
        "        projection_layer=LinearProjectionLayer(config['d_model'], config['target_vocab_size'])\n",
        "    )\n",
        "\n",
        "    # Initialize parameters\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "kdCt3wNi4EvY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤝 BREAKOUT ROOM #2:"
      ],
      "metadata": {
        "id": "6dn9RuqQEjQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking Flash Attention Against Naive Attention\n",
        "\n",
        "In this Breakout Room, we're going to explore the differences in Flash Attention and Naive Attention by way of benchmarking one against the other.\n",
        "\n",
        "We'll start with some verification helper functions to make sure things are working as expected.\n"
      ],
      "metadata": {
        "id": "BNjAS-LvV557"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Verification Functions\n",
        "\n",
        "Let's start by making sure we *can* use Flash Attention.\n",
        "\n",
        "Then confirm that we have access to a GPU environment - and then confirm we're running in reduced precision (required for Flash Attention)."
      ],
      "metadata": {
        "id": "-hjqEag6E53O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_flash_attn_available():\n",
        "    try:\n",
        "        import flash_attn\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "def verify_flash_attention(model):\n",
        "    \"\"\"Verify Flash Attention is actually being used\"\"\"\n",
        "    if not check_flash_attn_available():\n",
        "        print(\"Flash Attention is not available\")\n",
        "        return False\n",
        "\n",
        "    # Check if CUDA is available\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA is required for Flash Attention\")\n",
        "        return False\n",
        "\n",
        "    # Verify model precision\n",
        "    if not any(p.dtype == torch.float16 for p in model.parameters()):\n",
        "        print(\"Model needs to be in float16 for Flash Attention\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def profile_attention_call(model, input_data):\n",
        "    start_event = torch.cuda.Event(enable_timing=True)\n",
        "    end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start_event.record()\n",
        "\n",
        "    output = model(input_data)\n",
        "\n",
        "    end_event.record()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    return start_event.elapsed_time(end_event)"
      ],
      "metadata": {
        "id": "Xp6w1tnbdiOx"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Implement Benchmarking Suite\n",
        "\n",
        "This step should be easy enough - we just need to set up a class that can help us benchmark our model using the two forms of Attention we care about!"
      ],
      "metadata": {
        "id": "_NgRCa9FFOjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkResult:\n",
        "    time_ms: float\n",
        "\n",
        "class AttentionBenchmark:\n",
        "    def __init__(\n",
        "        self,\n",
        "        seq_lengths: List[int] = [128, 256, 512, 1024, 2048, 4096],\n",
        "        num_trials: int = 3,\n",
        "        num_warmup: int = 5,\n",
        "        num_iterations: int = 100,\n",
        "        device: str = \"cuda\"\n",
        "    ):\n",
        "        self.seq_lengths = seq_lengths\n",
        "        self.num_trials = num_trials\n",
        "        self.num_warmup = num_warmup\n",
        "        self.num_iterations = num_iterations\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def _clear_memory(self):\n",
        "        \"\"\"Clear GPU and CPU memory.\"\"\"\n",
        "        if self.device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    def _run_single_test(self, attention_type, seq_len: int, d_model: int = 256, num_heads: int = 4) -> Tuple[torch.Tensor, BenchmarkResult]:\n",
        "        \"\"\"Run a single test for the specified attention type and sequence length.\"\"\"\n",
        "        self._clear_memory()\n",
        "\n",
        "        if self.device.type == \"cuda\":\n",
        "            torch.cuda.reset_max_memory_allocated()\n",
        "\n",
        "        batch_size = 1\n",
        "\n",
        "        x = torch.randn(batch_size, seq_len, d_model).to(self.device)\n",
        "        mask = torch.ones(batch_size, 1, seq_len, seq_len).bool().to(self.device)\n",
        "\n",
        "        attn_config = AttentionConfig(\n",
        "            attention_type=attention_type,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        model = ModularAttention(attn_config).to(self.device)\n",
        "\n",
        "        if attention_type == AttentionType.FLASH:\n",
        "            if not check_flash_attn_available():\n",
        "                print(f\"Flash Attention not available for seq_len {seq_len}. Skipping...\")\n",
        "                return None, None\n",
        "            model = model.half()\n",
        "            x = x.half()\n",
        "            if not verify_flash_attention(model):\n",
        "                print(f\"Flash Attention verification failed for seq_len {seq_len}. Skipping...\")\n",
        "                return None, None\n",
        "\n",
        "        try:\n",
        "            for _ in range(self.num_warmup):\n",
        "                _ = model(x, x, x, mask)\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            total_time_ms = 0\n",
        "\n",
        "            for _ in range(self.num_iterations):\n",
        "                torch.cuda.synchronize()\n",
        "                start_time = time.perf_counter()\n",
        "\n",
        "                _ = model(x, x, x, mask)\n",
        "\n",
        "                torch.cuda.synchronize()\n",
        "                end_time = time.perf_counter()\n",
        "\n",
        "                iteration_time_ms = (end_time - start_time) * 1000\n",
        "                total_time_ms += iteration_time_ms\n",
        "\n",
        "            avg_time_ms = total_time_ms / self.num_iterations\n",
        "\n",
        "            del x, mask, model\n",
        "            self._clear_memory()\n",
        "\n",
        "            return None, BenchmarkResult(time_ms=avg_time_ms)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during {attention_type.value} test for seq_len {seq_len}: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "    def run_benchmark(self) -> pd.DataFrame:\n",
        "        \"\"\"Run the complete benchmark suite.\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for seq_len in tqdm(self.seq_lengths, desc=\"Testing sequence lengths\"):\n",
        "            batch_size = 4\n",
        "            trial_results = {attention_type: [] for attention_type in AttentionType}\n",
        "\n",
        "            for _ in range(self.num_trials):\n",
        "                for attention_type in AttentionType:\n",
        "                    _, result = self._run_single_test(\n",
        "                        attention_type=attention_type,\n",
        "                        seq_len=seq_len\n",
        "                    )\n",
        "                    if result is None:\n",
        "                        print(f\"Skipping {attention_type.value} for seq_len {seq_len} due to failure.\")\n",
        "                        continue\n",
        "\n",
        "                    trial_results[attention_type].append(result)\n",
        "\n",
        "                self._clear_memory()\n",
        "\n",
        "            for attention_type in AttentionType:\n",
        "                trial_data = trial_results[attention_type]\n",
        "                if not trial_data:\n",
        "                    continue\n",
        "\n",
        "                results.append({\n",
        "                    'Sequence Length': seq_len,\n",
        "                    'Attention Type': attention_type.value,\n",
        "                    'Batch Size': batch_size,\n",
        "                    'Time per Iteration (ms)': np.mean([r.time_ms for r in trial_data]),\n",
        "                    'Time Std Dev (ms)': np.std([r.time_ms for r in trial_data])\n",
        "                })\n",
        "\n",
        "            self._clear_memory()\n",
        "\n",
        "        if not results:\n",
        "            print(\"No successful benchmark results obtained.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "hfBcSHUo-MU6"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🏗️ Activity #1:\n",
        "\n",
        "Write out, in natural language, how this benchmark class works!\n",
        "\n",
        "### ANSWER\n",
        "- Class takes parameters like sequence lengths to test\n",
        "- Number of trials to run (3 by default), Number of warmup iterations (5 by default), Number of actual iterations (100 by default), Device selection (CUDA if available, else CPU)\n",
        "\n",
        "**Single test Run**\n",
        "- Before each test, it clears memory (both GPU and CPU)\n",
        "- Creates random input data with specified dimensions\n",
        "- Sets up the attention model with given configuration\n",
        "- For Flash Attention specifically, Checks availability, Converts to half precision and verifies functionality\n",
        "- Does warmup runs first to ensure stable timing\n",
        "- Measures time for multiple iterations of attention computation\n",
        "- Calculates average time per iteration\n",
        "- Cleans up memory after test\n",
        "\n",
        "**Full Benchmark**\n",
        "- Loops through all sequence lengths\n",
        "\n",
        "**For each length:**\n",
        "- Tests each attention type multiple times\n",
        "- Records timing results\n",
        "- Handles any failures gracefully\n",
        "- Calculates statistics (mean and standard deviation)\n",
        "- Returns results as a pandas DataFrame showing sequence length, attention type, batch size, average time per iteration and standard deviation of times"
      ],
      "metadata": {
        "id": "hEAe3A7yHHQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Run Benchmark!\n",
        "\n",
        "Easily enough, all we need to do is actually fire the benchmark off!"
      ],
      "metadata": {
        "id": "iktRcvIrFZqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark = AttentionBenchmark(\n",
        "    seq_lengths=[128, 256, 512, 1024, 2048, 4096, 8192, 16392],\n",
        "    num_trials=5,\n",
        ")\n",
        "results_df = benchmark.run_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whjxtCWUL6po",
        "outputId": "a292de35-145c-4782-d2dd-2c6f5d9afc49"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTesting sequence lengths:   0%|          | 0/8 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "Testing sequence lengths: 100%|██████████| 8/8 [01:11<00:00,  8.88s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4: Plotting the Results\n",
        "\n",
        "Now that we have a benchmark created - we'll use Plotly to...well, plot our results!"
      ],
      "metadata": {
        "id": "8P40ubk9FfHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_benchmark_results(df: pd.DataFrame) -> go.Figure:\n",
        "    \"\"\"Visualize benchmark results focusing on time per iteration.\"\"\"\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Colors for different attention types\n",
        "    colors = {\n",
        "        'regular': 'blue',\n",
        "        'flash': 'red'\n",
        "    }\n",
        "\n",
        "    # Plot: Time per Iteration vs Sequence Length\n",
        "    for att_type in df['Attention Type'].unique():\n",
        "        data = df[df['Attention Type'] == att_type]\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=data['Sequence Length'],\n",
        "                y=data['Time per Iteration (ms)'],\n",
        "                name=f\"{att_type.title()} Time\",\n",
        "                line=dict(color=colors.get(att_type, 'black')),\n",
        "                mode='lines+markers'\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title='Attention Mechanism Benchmark Results',\n",
        "        xaxis_title='Sequence Length',\n",
        "        yaxis_title='Time per Iteration (ms)',\n",
        "        height=600,\n",
        "        showlegend=True,\n",
        "        hovermode='x unified'\n",
        "    )\n",
        "\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "FhQrcBi6L2m6"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plot_benchmark_results(results_df)\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "kuthp_0CYw2V",
        "outputId": "08978a47-d10c-461c-dec3-950b09be9480"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"2363fd46-4aac-4e35-9053-54003ab44c88\" class=\"plotly-graph-div\" style=\"height:600px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2363fd46-4aac-4e35-9053-54003ab44c88\")) {                    Plotly.newPlot(                        \"2363fd46-4aac-4e35-9053-54003ab44c88\",                        [{\"line\":{\"color\":\"blue\"},\"mode\":\"lines+markers\",\"name\":\"Regular Time\",\"x\":[128,256,512,1024,2048,4096,8192,16392],\"y\":[0.621538627992777,0.5909622359954483,0.6304673739955433,0.7708245739931954,1.490953868005363,4.651171057993906,15.916948849995151,59.925457771990295],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\"},\"mode\":\"lines+markers\",\"name\":\"Flash Time\",\"x\":[128,256,512,1024,2048,4096,8192,16392],\"y\":[0.6362177140063068,0.6351918719947207,0.6372214619977967,0.5932752519943278,0.631568120000793,0.6471851639989836,0.820044305995907,2.009687870000562],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Attention Mechanism Benchmark Results\"},\"xaxis\":{\"title\":{\"text\":\"Sequence Length\"}},\"yaxis\":{\"title\":{\"text\":\"Time per Iteration (ms)\"}},\"height\":600,\"showlegend\":true,\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2363fd46-4aac-4e35-9053-54003ab44c88');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👪❓Discussion Question 3:\n",
        "\n",
        "Explore with your group what these results show!\n",
        "\n",
        "**Regular Attention**\n",
        "- Shows quadratic growth, performance degrades significantly with longer sequences\n",
        "- At 16k tokens, takes around 60ms per iteration\n",
        "\n",
        "**Flash Attention**\n",
        "- Remains almost flat/linear even as sequence length increases, consistently stays around 2-3ms per iteration\n",
        "- Shows remarkable efficiency even at 16k tokens\n",
        "\n",
        "**Conclusion**:Flash attention makes handling long sequences much more computationally feasible\n"
      ],
      "metadata": {
        "id": "mwaF3aa2GZNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Acknowledgements\n",
        "\n",
        "This notebook is heavily adapted from a number of incredible resources on Transformers, including but not limited to:\n",
        "\n",
        "- https://blog.floydhub.com/the-transformer-in-pytorch/\n",
        "- https://arxiv.org/pdf/1706.03762.pdf\n",
        "- https://txt.cohere.com/what-are-transformer-models/\n",
        "- https://jalammar.github.io/illustrated-transformer/"
      ],
      "metadata": {
        "id": "Z9Cofqp53bQB"
      }
    }
  ]
}